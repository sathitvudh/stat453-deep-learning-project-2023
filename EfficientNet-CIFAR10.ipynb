{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ccb35d98",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from math import ceil\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb3f2466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Image Batch Dimensions: torch.Size([128, 3, 32, 32])\n",
      "Image Label Dimensions: torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "####################################\n",
    "### CIFAR-10 DATASET\n",
    "####################################\n",
    "\n",
    "# Scale inout images to 0-1 range using transforms.ToTensor()\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "idx = np.arange(50000)\n",
    "np.random.shuffle(idx)\n",
    "val_idx, train_idx = idx[:1000], idx[1000:]\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "val_sampler = SubsetRandomSampler(val_idx)\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root = 'data',\n",
    "                                 train = True,\n",
    "                                 transform = transforms.ToTensor(),\n",
    "                                 download = True)\n",
    "\n",
    "test_dataset = datasets.CIFAR10(root = 'data',\n",
    "                                 train = False,\n",
    "                                 transform = transforms.ToTensor())\n",
    "\n",
    "train_loader = DataLoader(dataset = train_dataset,\n",
    "                         batch_size = BATCH_SIZE,\n",
    "                         #shuffle = True,\n",
    "                         sampler = train_sampler)\n",
    "\n",
    "val_loader = DataLoader(dataset = train_dataset,\n",
    "                       batch_size = BATCH_SIZE,\n",
    "                       sampler = val_sampler)\n",
    "\n",
    "test_loader = DataLoader(dataset = test_dataset,\n",
    "                         batch_size = BATCH_SIZE,\n",
    "                         shuffle = False)\n",
    "\n",
    "# Checking the dataset\n",
    "for images, labels in train_loader:\n",
    "    print('Image Batch Dimensions:', images.shape)\n",
    "    print('Image Label Dimensions:', labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a94e329c",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "### SETTINGS\n",
    "####################################\n",
    "\n",
    "# Initializing expand_ratio, channels, repeats, stride, kernel_size\n",
    "# for base \n",
    "\n",
    "base_model = [\n",
    "    [1, 16, 1, 1, 3],\n",
    "    [6, 24, 2, 2, 3],\n",
    "    [6, 40, 2, 2, 5],\n",
    "    [6, 80, 3, 2, 3],\n",
    "    [6, 112, 3, 1, 5],\n",
    "    [6, 192, 4, 2, 5],\n",
    "    [6, 320, 1, 1, 3]\n",
    "]\n",
    "\n",
    "# Initializing config for each verion (phi_value, resolution, drop_rate)\n",
    "version_config = {\n",
    "    'b0': (0, 224, 0.2),\n",
    "    'b1': (0.5, 240, 0.2),\n",
    "    'b2': (1, 260, 0.3),\n",
    "    'b3': (2, 300, 0.3),\n",
    "    'b4': (3, 380, 0.4),\n",
    "    'b5': (4, 456, 0.4)\n",
    "}\n",
    "\n",
    "# Hyperparameter setting\n",
    "RANDOM_SEED = 1\n",
    "NUM_CLASSES = 10 # using CIFAR-10\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 20\n",
    "version = \"b0\"\n",
    "phi, res, drop_rate = version_config[version]\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "####################################\n",
    "### LEARNING RATE SETTINGS\n",
    "####################################\n",
    "\n",
    "iter_per_ep = len(train_loader.sampler.indices) // train_loader.batch_size\n",
    "base_lr = 0.01\n",
    "max_lr = 0.1\n",
    "batch_step = -1\n",
    "cur_lr = base_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a8e0548a",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "### MODEL\n",
    "####################################\n",
    "\n",
    "# 1. Start with Convolutional Neural Network block\n",
    "# Con -> Batch_norm -> ReLU\n",
    "class CNN_block(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, kernel_size,\n",
    "                 stride, padding, groups = 1):\n",
    "        \n",
    "        super(CNN_block, self).__init__()\n",
    "        self.cnn = nn.Conv2d(in_ch, out_ch, kernel_size,\n",
    "                             stride, padding, groups = groups,\n",
    "                             bias = False) # If we set group = 1 -> normal conv, groups = in_ch -> Depthwise conv\n",
    "        self.batch_norm = nn.BatchNorm2d(out_ch)\n",
    "        self.silu = nn.SiLU() # SiLU == Swish\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        out = self.cnn(x)\n",
    "        out = self.batch_norm(out)\n",
    "        out = self.silu(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "# 2. Squeeze Excitation trick used inside Inverted Res block\n",
    "class Squeeze_Excitation(nn.Module):\n",
    "    def __init__(self, in_ch, reduced_dim):\n",
    "        \n",
    "        super(Squeeze_Excitation, self).__init__()\n",
    "        self.se = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1), # C x H x W -> C x 1 x 1\n",
    "            nn.Conv2d(in_ch, reduced_dim, 1),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(reduced_dim, in_ch, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x * self.se(x) # each channel, multiply the values from sequential (How much we should prioritize the channels)\n",
    "\n",
    "# 3. Inverted Residual Block\n",
    "class Inverted_Residual_block(nn.Module):\n",
    "    # expand_ratio takes inputs and expands to higher number of channels\n",
    "    # reduce by 1/4 (for squeeze excitation)\n",
    "    # survival_prob is for stochastic depth\n",
    "    def __init__(self, in_ch, out_ch, kernel_size,\n",
    "                stride, padding, expand_ratio, reduction = 4,\n",
    "                survival_prob = 0.8):\n",
    "        \n",
    "        super(Inverted_Residual_block, self).__init__()\n",
    "        self.survival_prob = 0.8\n",
    "        self.use_residual = in_ch == out_ch and stride == 1\n",
    "        hidden_dim = in_ch * expand_ratio\n",
    "        self.expand = in_ch != hidden_dim\n",
    "        reduced_dim = int(in_ch/reduction)\n",
    "        \n",
    "        if self.expand:\n",
    "            self.expand_conv = CNN_block(in_ch,\n",
    "                                        hidden_dim,\n",
    "                                        kernel_size = 3,\n",
    "                                        stride = 1,\n",
    "                                        padding = 1)\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            CNN_block(\n",
    "                hidden_dim, hidden_dim, kernel_size, stride,\n",
    "                padding, groups = hidden_dim\n",
    "            ),\n",
    "            Squeeze_Excitation(hidden_dim, reduced_dim),\n",
    "            nn.Conv2d(hidden_dim, out_ch, 1, bias = False),\n",
    "            nn.BatchNorm2d(out_ch)\n",
    "        )\n",
    "    \n",
    "    def Stochastic_Depth(self, x):\n",
    "        if not self.training: # like-dropout, randomly remove certain layer\n",
    "            return x\n",
    "        # compute value 0 or 1 for each example\n",
    "        binary_tensor = torch.rand(x.shape[0], 1, 1, 1, device = x.device) < self.survival_prob\n",
    "        \n",
    "        return torch.div(x, self.survival_prob) * binary_tensor # try to maintain mean and sd in the batch\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        out = self.expand_conv(x) if self.expand else x\n",
    "        \n",
    "        if self.use_residual:\n",
    "            return self.Stochastic_Depth(self.conv(out)) + x\n",
    "        else: # if we down sampled or if the channel changed\n",
    "            return self.conv(out)\n",
    "        \n",
    "class EfficientNet(nn.Module):\n",
    "    def __init__(self, version, num_classes):\n",
    "        super(EfficientNet, self).__init__()\n",
    "        width_factor, depth_factor, dropout_rate = self.calculate_factors(version)\n",
    "        last_channels = ceil(1280 * width_factor)\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.features = self.feature_extraction(width_factor, depth_factor, last_channels)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(last_channels, num_classes)\n",
    "        )\n",
    "        \n",
    "    def calculate_factors(self, version, alpha = 1.2, beta = 1.1):\n",
    "        phi, resolution, drop_rate = version_config[version]\n",
    "        depth_factor = alpha ** phi # how many layers we should increase for each stage\n",
    "        width_factor = beta ** phi # how much larger the channels should be\n",
    "        return depth_factor, width_factor, drop_rate\n",
    "    \n",
    "    def feature_extraction(self, width_factor, depth_factor, last_channels):\n",
    "        channels = int(32 * width_factor)\n",
    "        features = [CNN_block(3, channels, 3, stride = 2, padding = 1)]\n",
    "        in_ch = channels\n",
    "        \n",
    "        for expand_ratio, channels, repeats, stride, kernel_size in base_model:\n",
    "            out_ch = 4 *ceil(int(channels*width_factor)/4) # modulus of 4 since we identify reduction = 4 in SE\n",
    "            layers_repeats = ceil(repeats * depth_factor)\n",
    "            \n",
    "            for layer in range(layers_repeats):\n",
    "                features.append(\n",
    "                    Inverted_Residual_block(\n",
    "                        in_ch,\n",
    "                        out_ch,\n",
    "                        expand_ratio = expand_ratio,\n",
    "                        stride = stride if layer == 0 else 1,\n",
    "                        kernel_size = kernel_size,\n",
    "                        padding = kernel_size // 2, # if k=1 -> pad=0, k=3 -> pad=1, k=5-> pad=2\n",
    "                    )\n",
    "                )\n",
    "                in_ch = out_ch\n",
    "        features.append(\n",
    "            CNN_block(in_ch, last_channels, kernel_size = 1,\n",
    "                     stride = 1, padding = 0)\n",
    "        )\n",
    "        \n",
    "        return nn.Sequential(*features)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        out = self.features(x)\n",
    "        out = self.pool(out)\n",
    "        \n",
    "        return self.classifier(out.view(out.shape[0], -1))\n",
    "    \n",
    "def test():\n",
    "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    version = \"b0\"\n",
    "    phi, res, drop_rate = version_config[version]\n",
    "    num_examples, num_classes = 4,10\n",
    "    x = torch.randn((num_examples, 3, res, res)).to(DEVICE)\n",
    "    model = EfficientNet(version = version,\n",
    "                        num_classes = num_classes).to(DEVICE)\n",
    "    \n",
    "    print(model(x).shape) # (num_examples, num_classes)\n",
    "    \n",
    "#test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f8030c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "### CYCLICAL LEARNING RATE\n",
    "####################################\n",
    "\n",
    "def cyclical_learning_rate(batch_step,\n",
    "                           step_size,\n",
    "                           base_lr=0.001,\n",
    "                           max_lr=0.006,\n",
    "                           mode='triangular',\n",
    "                           gamma=0.999995):\n",
    "\n",
    "    cycle = np.floor(1 + batch_step / (2. * step_size))\n",
    "    x = np.abs(batch_step / float(step_size) - 2 * cycle + 1)\n",
    "\n",
    "    lr_delta = (max_lr - base_lr) * np.maximum(0, (1 - x))\n",
    "    \n",
    "    if mode == 'triangular':\n",
    "        pass\n",
    "    elif mode == 'triangular2':\n",
    "        lr_delta = lr_delta * 1 / (2. ** (cycle - 1))\n",
    "    elif mode == 'exp_range':\n",
    "        lr_delta = lr_delta * (gamma**(batch_step))\n",
    "    else:\n",
    "        raise ValueError('mode must be \"triangular\", \"triangular2\", or \"exp_range\"')\n",
    "        \n",
    "    lr = base_lr + lr_delta\n",
    "    \n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c859e119",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "########################\n",
    "### COST AND OPTIMIZER\n",
    "########################\n",
    "\n",
    "model = EfficientNet(version = version, num_classes = NUM_CLASSES).to(DEVICE)\n",
    "model.to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = base_lr, momentum = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf9b822b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/020 | Batch 000/383 | Cost: 2.3416\n",
      "Epoch: 001/020 | Batch 020/383 | Cost: 2.1889\n",
      "Epoch: 001/020 | Batch 040/383 | Cost: 1.9202\n",
      "Epoch: 001/020 | Batch 060/383 | Cost: 2.0684\n",
      "Epoch: 001/020 | Batch 080/383 | Cost: 2.1312\n",
      "Epoch: 001/020 | Batch 100/383 | Cost: 2.0611\n",
      "Epoch: 001/020 | Batch 120/383 | Cost: 1.7996\n",
      "Epoch: 001/020 | Batch 140/383 | Cost: 1.8685\n",
      "Epoch: 001/020 | Batch 160/383 | Cost: 1.9922\n",
      "Epoch: 001/020 | Batch 180/383 | Cost: 2.0280\n",
      "Epoch: 001/020 | Batch 200/383 | Cost: 1.8682\n",
      "Epoch: 001/020 | Batch 220/383 | Cost: 2.0071\n",
      "Epoch: 001/020 | Batch 240/383 | Cost: 1.6610\n",
      "Epoch: 001/020 | Batch 260/383 | Cost: 1.6914\n",
      "Epoch: 001/020 | Batch 280/383 | Cost: 2.3245\n",
      "Epoch: 001/020 | Batch 300/383 | Cost: 2.1228\n",
      "Epoch: 001/020 | Batch 320/383 | Cost: 2.0548\n",
      "Epoch: 001/020 | Batch 340/383 | Cost: 1.9390\n",
      "Epoch: 001/020 | Batch 360/383 | Cost: 2.0801\n",
      "Epoch: 001/020 | Batch 380/383 | Cost: 2.1247\n",
      "base_lr:  0.01\n",
      "Epoch: 001/020 Train Acc.: 30.95% | Validation Acc.: 28.70%\n",
      "Time elapsed: 13.73 min\n",
      "Epoch: 002/020 | Batch 000/383 | Cost: 2.0465\n",
      "Epoch: 002/020 | Batch 020/383 | Cost: 2.1168\n",
      "Epoch: 002/020 | Batch 040/383 | Cost: 1.9009\n",
      "Epoch: 002/020 | Batch 060/383 | Cost: 1.7341\n",
      "Epoch: 002/020 | Batch 080/383 | Cost: 1.8021\n",
      "Epoch: 002/020 | Batch 100/383 | Cost: 1.8507\n",
      "Epoch: 002/020 | Batch 120/383 | Cost: 1.7768\n",
      "Epoch: 002/020 | Batch 140/383 | Cost: 1.6760\n",
      "Epoch: 002/020 | Batch 160/383 | Cost: 1.6861\n",
      "Epoch: 002/020 | Batch 180/383 | Cost: 1.8394\n",
      "Epoch: 002/020 | Batch 200/383 | Cost: 1.4969\n",
      "Epoch: 002/020 | Batch 220/383 | Cost: 1.7069\n",
      "Epoch: 002/020 | Batch 240/383 | Cost: 1.7078\n",
      "Epoch: 002/020 | Batch 260/383 | Cost: 1.7582\n",
      "Epoch: 002/020 | Batch 280/383 | Cost: 1.8448\n",
      "Epoch: 002/020 | Batch 300/383 | Cost: 1.5316\n",
      "Epoch: 002/020 | Batch 320/383 | Cost: 1.3918\n",
      "Epoch: 002/020 | Batch 340/383 | Cost: 1.3899\n",
      "Epoch: 002/020 | Batch 360/383 | Cost: 1.6132\n",
      "Epoch: 002/020 | Batch 380/383 | Cost: 1.5260\n",
      "base_lr:  0.014641361256544508\n",
      "Epoch: 002/020 Train Acc.: 46.69% | Validation Acc.: 46.50%\n",
      "Time elapsed: 27.81 min\n",
      "Epoch: 003/020 | Batch 000/383 | Cost: 1.8508\n",
      "Epoch: 003/020 | Batch 020/383 | Cost: 1.7506\n",
      "Epoch: 003/020 | Batch 040/383 | Cost: 1.7604\n",
      "Epoch: 003/020 | Batch 060/383 | Cost: 2.1385\n",
      "Epoch: 003/020 | Batch 080/383 | Cost: 1.9640\n",
      "Epoch: 003/020 | Batch 100/383 | Cost: 1.7386\n",
      "Epoch: 003/020 | Batch 120/383 | Cost: 1.8355\n",
      "Epoch: 003/020 | Batch 140/383 | Cost: 1.8491\n",
      "Epoch: 003/020 | Batch 160/383 | Cost: 1.5253\n",
      "Epoch: 003/020 | Batch 180/383 | Cost: 1.6616\n",
      "Epoch: 003/020 | Batch 200/383 | Cost: 1.7481\n",
      "Epoch: 003/020 | Batch 220/383 | Cost: 1.6010\n",
      "Epoch: 003/020 | Batch 240/383 | Cost: 1.6372\n",
      "Epoch: 003/020 | Batch 260/383 | Cost: 1.7516\n",
      "Epoch: 003/020 | Batch 280/383 | Cost: 1.5582\n",
      "Epoch: 003/020 | Batch 300/383 | Cost: 1.4798\n",
      "Epoch: 003/020 | Batch 320/383 | Cost: 1.7404\n",
      "Epoch: 003/020 | Batch 340/383 | Cost: 1.9268\n",
      "Epoch: 003/020 | Batch 360/383 | Cost: 1.8052\n",
      "Epoch: 003/020 | Batch 380/383 | Cost: 1.8812\n",
      "base_lr:  0.02332246888791427\n",
      "Epoch: 003/020 Train Acc.: 26.13% | Validation Acc.: 25.50%\n",
      "Time elapsed: 41.84 min\n",
      "Epoch: 004/020 | Batch 000/383 | Cost: 1.8691\n",
      "Epoch: 004/020 | Batch 020/383 | Cost: 1.9683\n",
      "Epoch: 004/020 | Batch 040/383 | Cost: 1.9398\n",
      "Epoch: 004/020 | Batch 060/383 | Cost: 1.8177\n",
      "Epoch: 004/020 | Batch 080/383 | Cost: 1.6887\n",
      "Epoch: 004/020 | Batch 100/383 | Cost: 1.6281\n",
      "Epoch: 004/020 | Batch 120/383 | Cost: 1.8264\n",
      "Epoch: 004/020 | Batch 140/383 | Cost: 1.7691\n",
      "Epoch: 004/020 | Batch 160/383 | Cost: 1.7078\n",
      "Epoch: 004/020 | Batch 180/383 | Cost: 1.6559\n",
      "Epoch: 004/020 | Batch 200/383 | Cost: 1.7094\n",
      "Epoch: 004/020 | Batch 220/383 | Cost: 1.6650\n",
      "Epoch: 004/020 | Batch 240/383 | Cost: 1.4062\n",
      "Epoch: 004/020 | Batch 260/383 | Cost: 1.6799\n",
      "Epoch: 004/020 | Batch 280/383 | Cost: 1.7294\n",
      "Epoch: 004/020 | Batch 300/383 | Cost: 1.6817\n",
      "Epoch: 004/020 | Batch 320/383 | Cost: 1.5435\n",
      "Epoch: 004/020 | Batch 340/383 | Cost: 1.5848\n",
      "Epoch: 004/020 | Batch 360/383 | Cost: 1.4308\n",
      "Epoch: 004/020 | Batch 380/383 | Cost: 1.5820\n",
      "base_lr:  0.03496460711959222\n",
      "Epoch: 004/020 Train Acc.: 49.25% | Validation Acc.: 47.90%\n",
      "Time elapsed: 55.92 min\n",
      "Epoch: 005/020 | Batch 000/383 | Cost: 1.4028\n",
      "Epoch: 005/020 | Batch 020/383 | Cost: 1.4951\n",
      "Epoch: 005/020 | Batch 040/383 | Cost: 1.5775\n",
      "Epoch: 005/020 | Batch 060/383 | Cost: 1.4721\n",
      "Epoch: 005/020 | Batch 080/383 | Cost: 1.4703\n",
      "Epoch: 005/020 | Batch 100/383 | Cost: 1.4096\n",
      "Epoch: 005/020 | Batch 120/383 | Cost: 1.4827\n",
      "Epoch: 005/020 | Batch 140/383 | Cost: 1.3759\n",
      "Epoch: 005/020 | Batch 160/383 | Cost: 1.4517\n",
      "Epoch: 005/020 | Batch 180/383 | Cost: 1.4283\n",
      "Epoch: 005/020 | Batch 200/383 | Cost: 1.5312\n",
      "Epoch: 005/020 | Batch 220/383 | Cost: 1.2533\n",
      "Epoch: 005/020 | Batch 240/383 | Cost: 1.4871\n",
      "Epoch: 005/020 | Batch 260/383 | Cost: 1.1573\n",
      "Epoch: 005/020 | Batch 280/383 | Cost: 1.2407\n",
      "Epoch: 005/020 | Batch 300/383 | Cost: 1.6058\n",
      "Epoch: 005/020 | Batch 320/383 | Cost: 1.2188\n",
      "Epoch: 005/020 | Batch 340/383 | Cost: 1.3487\n",
      "Epoch: 005/020 | Batch 360/383 | Cost: 1.3971\n",
      "Epoch: 005/020 | Batch 380/383 | Cost: 1.4231\n",
      "base_lr:  0.048099372985360433\n",
      "Epoch: 005/020 Train Acc.: 53.89% | Validation Acc.: 51.90%\n",
      "Time elapsed: 70.21 min\n",
      "Epoch: 006/020 | Batch 000/383 | Cost: 1.2504\n",
      "Epoch: 006/020 | Batch 020/383 | Cost: 1.2350\n",
      "Epoch: 006/020 | Batch 040/383 | Cost: 1.3121\n",
      "Epoch: 006/020 | Batch 060/383 | Cost: 1.2058\n",
      "Epoch: 006/020 | Batch 080/383 | Cost: 1.3549\n",
      "Epoch: 006/020 | Batch 100/383 | Cost: 1.2485\n",
      "Epoch: 006/020 | Batch 120/383 | Cost: 1.3342\n",
      "Epoch: 006/020 | Batch 140/383 | Cost: 1.3023\n",
      "Epoch: 006/020 | Batch 160/383 | Cost: 1.3705\n",
      "Epoch: 006/020 | Batch 180/383 | Cost: 1.0883\n",
      "Epoch: 006/020 | Batch 200/383 | Cost: 1.3730\n",
      "Epoch: 006/020 | Batch 220/383 | Cost: 1.7641\n",
      "Epoch: 006/020 | Batch 240/383 | Cost: 1.4045\n",
      "Epoch: 006/020 | Batch 260/383 | Cost: 1.3898\n",
      "Epoch: 006/020 | Batch 280/383 | Cost: 1.4633\n",
      "Epoch: 006/020 | Batch 300/383 | Cost: 1.3796\n",
      "Epoch: 006/020 | Batch 320/383 | Cost: 1.1958\n",
      "Epoch: 006/020 | Batch 340/383 | Cost: 1.2937\n",
      "Epoch: 006/020 | Batch 360/383 | Cost: 1.2293\n",
      "Epoch: 006/020 | Batch 380/383 | Cost: 1.3152\n",
      "base_lr:  0.06118322215161642\n",
      "Epoch: 006/020 Train Acc.: 56.32% | Validation Acc.: 53.10%\n",
      "Time elapsed: 84.05 min\n",
      "Epoch: 007/020 | Batch 000/383 | Cost: 1.3034\n",
      "Epoch: 007/020 | Batch 020/383 | Cost: 1.1312\n",
      "Epoch: 007/020 | Batch 040/383 | Cost: 1.1146\n",
      "Epoch: 007/020 | Batch 060/383 | Cost: 1.1437\n",
      "Epoch: 007/020 | Batch 080/383 | Cost: 1.1762\n",
      "Epoch: 007/020 | Batch 100/383 | Cost: 1.1710\n",
      "Epoch: 007/020 | Batch 120/383 | Cost: 1.2063\n",
      "Epoch: 007/020 | Batch 140/383 | Cost: 1.0942\n",
      "Epoch: 007/020 | Batch 160/383 | Cost: 1.1968\n",
      "Epoch: 007/020 | Batch 180/383 | Cost: 1.1920\n",
      "Epoch: 007/020 | Batch 200/383 | Cost: 1.1445\n",
      "Epoch: 007/020 | Batch 220/383 | Cost: 1.2121\n",
      "Epoch: 007/020 | Batch 240/383 | Cost: 1.2450\n",
      "Epoch: 007/020 | Batch 260/383 | Cost: 1.1301\n",
      "Epoch: 007/020 | Batch 280/383 | Cost: 1.3044\n",
      "Epoch: 007/020 | Batch 300/383 | Cost: 1.2251\n",
      "Epoch: 007/020 | Batch 320/383 | Cost: 1.1516\n",
      "Epoch: 007/020 | Batch 340/383 | Cost: 1.1798\n",
      "Epoch: 007/020 | Batch 360/383 | Cost: 1.2474\n",
      "Epoch: 007/020 | Batch 380/383 | Cost: 1.4088\n",
      "base_lr:  0.07291462791757423\n",
      "Epoch: 007/020 Train Acc.: 56.74% | Validation Acc.: 54.60%\n",
      "Time elapsed: 97.95 min\n",
      "Epoch: 008/020 | Batch 000/383 | Cost: 1.1771\n",
      "Epoch: 008/020 | Batch 020/383 | Cost: 1.0784\n",
      "Epoch: 008/020 | Batch 040/383 | Cost: 1.2595\n",
      "Epoch: 008/020 | Batch 060/383 | Cost: 1.2626\n",
      "Epoch: 008/020 | Batch 080/383 | Cost: 1.1452\n",
      "Epoch: 008/020 | Batch 100/383 | Cost: 1.1297\n",
      "Epoch: 008/020 | Batch 120/383 | Cost: 1.1095\n",
      "Epoch: 008/020 | Batch 140/383 | Cost: 1.1057\n",
      "Epoch: 008/020 | Batch 160/383 | Cost: 1.1898\n",
      "Epoch: 008/020 | Batch 180/383 | Cost: 1.0423\n",
      "Epoch: 008/020 | Batch 200/383 | Cost: 1.1138\n",
      "Epoch: 008/020 | Batch 220/383 | Cost: 0.9183\n",
      "Epoch: 008/020 | Batch 240/383 | Cost: 1.0414\n",
      "Epoch: 008/020 | Batch 260/383 | Cost: 0.9459\n",
      "Epoch: 008/020 | Batch 280/383 | Cost: 1.1076\n",
      "Epoch: 008/020 | Batch 300/383 | Cost: 1.0137\n",
      "Epoch: 008/020 | Batch 320/383 | Cost: 0.9707\n",
      "Epoch: 008/020 | Batch 340/383 | Cost: 1.0092\n",
      "Epoch: 008/020 | Batch 360/383 | Cost: 1.0754\n",
      "Epoch: 008/020 | Batch 380/383 | Cost: 1.0609\n",
      "base_lr:  0.08245832185028237\n",
      "Epoch: 008/020 Train Acc.: 65.27% | Validation Acc.: 59.40%\n",
      "Time elapsed: 111.79 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 009/020 | Batch 000/383 | Cost: 0.8487\n",
      "Epoch: 009/020 | Batch 020/383 | Cost: 0.9426\n",
      "Epoch: 009/020 | Batch 040/383 | Cost: 1.2127\n",
      "Epoch: 009/020 | Batch 060/383 | Cost: 1.0980\n",
      "Epoch: 009/020 | Batch 080/383 | Cost: 1.1473\n",
      "Epoch: 009/020 | Batch 100/383 | Cost: 1.0826\n",
      "Epoch: 009/020 | Batch 120/383 | Cost: 1.3110\n",
      "Epoch: 009/020 | Batch 140/383 | Cost: 0.9517\n",
      "Epoch: 009/020 | Batch 160/383 | Cost: 1.1325\n",
      "Epoch: 009/020 | Batch 180/383 | Cost: 1.1004\n",
      "Epoch: 009/020 | Batch 200/383 | Cost: 1.0676\n",
      "Epoch: 009/020 | Batch 220/383 | Cost: 0.9777\n",
      "Epoch: 009/020 | Batch 240/383 | Cost: 0.8449\n",
      "Epoch: 009/020 | Batch 260/383 | Cost: 1.0518\n",
      "Epoch: 009/020 | Batch 280/383 | Cost: 0.9790\n",
      "Epoch: 009/020 | Batch 300/383 | Cost: 0.9476\n",
      "Epoch: 009/020 | Batch 320/383 | Cost: 0.9192\n",
      "Epoch: 009/020 | Batch 340/383 | Cost: 0.9014\n",
      "Epoch: 009/020 | Batch 360/383 | Cost: 0.8830\n",
      "Epoch: 009/020 | Batch 380/383 | Cost: 0.9383\n",
      "base_lr:  0.08951861770242658\n",
      "Epoch: 009/020 Train Acc.: 67.77% | Validation Acc.: 63.30%\n",
      "Time elapsed: 125.60 min\n",
      "Epoch: 010/020 | Batch 000/383 | Cost: 1.0346\n",
      "Epoch: 010/020 | Batch 020/383 | Cost: 1.0336\n",
      "Epoch: 010/020 | Batch 040/383 | Cost: 1.0578\n",
      "Epoch: 010/020 | Batch 060/383 | Cost: 0.7744\n",
      "Epoch: 010/020 | Batch 080/383 | Cost: 0.8565\n",
      "Epoch: 010/020 | Batch 100/383 | Cost: 0.9130\n",
      "Epoch: 010/020 | Batch 120/383 | Cost: 0.7881\n",
      "Epoch: 010/020 | Batch 140/383 | Cost: 0.8900\n",
      "Epoch: 010/020 | Batch 160/383 | Cost: 0.9646\n",
      "Epoch: 010/020 | Batch 180/383 | Cost: 0.9372\n",
      "Epoch: 010/020 | Batch 200/383 | Cost: 0.9798\n",
      "Epoch: 010/020 | Batch 220/383 | Cost: 0.9307\n",
      "Epoch: 010/020 | Batch 240/383 | Cost: 0.9730\n",
      "Epoch: 010/020 | Batch 260/383 | Cost: 0.7923\n",
      "Epoch: 010/020 | Batch 280/383 | Cost: 1.0166\n",
      "Epoch: 010/020 | Batch 300/383 | Cost: 0.9717\n",
      "Epoch: 010/020 | Batch 320/383 | Cost: 0.7945\n",
      "Epoch: 010/020 | Batch 340/383 | Cost: 0.9936\n",
      "Epoch: 010/020 | Batch 360/383 | Cost: 0.9151\n",
      "Epoch: 010/020 | Batch 380/383 | Cost: 0.8703\n",
      "base_lr:  0.09426267790988849\n",
      "Epoch: 010/020 Train Acc.: 73.21% | Validation Acc.: 65.70%\n",
      "Time elapsed: 139.46 min\n",
      "Epoch: 011/020 | Batch 000/383 | Cost: 0.8777\n",
      "Epoch: 011/020 | Batch 020/383 | Cost: 1.0031\n",
      "Epoch: 011/020 | Batch 040/383 | Cost: 0.9392\n",
      "Epoch: 011/020 | Batch 060/383 | Cost: 1.0614\n",
      "Epoch: 011/020 | Batch 080/383 | Cost: 0.8271\n",
      "Epoch: 011/020 | Batch 100/383 | Cost: 1.0562\n",
      "Epoch: 011/020 | Batch 120/383 | Cost: 0.8730\n",
      "Epoch: 011/020 | Batch 140/383 | Cost: 0.8480\n",
      "Epoch: 011/020 | Batch 160/383 | Cost: 0.8466\n",
      "Epoch: 011/020 | Batch 180/383 | Cost: 0.8397\n",
      "Epoch: 011/020 | Batch 200/383 | Cost: 0.7517\n",
      "Epoch: 011/020 | Batch 220/383 | Cost: 0.7949\n",
      "Epoch: 011/020 | Batch 240/383 | Cost: 0.7929\n",
      "Epoch: 011/020 | Batch 260/383 | Cost: 0.8239\n",
      "Epoch: 011/020 | Batch 280/383 | Cost: 0.8050\n",
      "Epoch: 011/020 | Batch 300/383 | Cost: 0.9498\n",
      "Epoch: 011/020 | Batch 320/383 | Cost: 0.6277\n",
      "Epoch: 011/020 | Batch 340/383 | Cost: 0.7585\n",
      "Epoch: 011/020 | Batch 360/383 | Cost: 0.9403\n",
      "Epoch: 011/020 | Batch 380/383 | Cost: 1.0491\n",
      "base_lr:  0.09714710908110817\n",
      "Epoch: 011/020 Train Acc.: 74.18% | Validation Acc.: 67.10%\n",
      "Time elapsed: 166.70 min\n",
      "Epoch: 012/020 | Batch 000/383 | Cost: 0.7789\n",
      "Epoch: 012/020 | Batch 020/383 | Cost: 0.6864\n",
      "Epoch: 012/020 | Batch 040/383 | Cost: 0.8045\n",
      "Epoch: 012/020 | Batch 060/383 | Cost: 0.6753\n",
      "Epoch: 012/020 | Batch 080/383 | Cost: 0.7969\n",
      "Epoch: 012/020 | Batch 100/383 | Cost: 0.6255\n",
      "Epoch: 012/020 | Batch 120/383 | Cost: 0.8888\n",
      "Epoch: 012/020 | Batch 140/383 | Cost: 0.7513\n",
      "Epoch: 012/020 | Batch 160/383 | Cost: 0.6664\n",
      "Epoch: 012/020 | Batch 180/383 | Cost: 0.9374\n",
      "Epoch: 012/020 | Batch 200/383 | Cost: 1.1771\n",
      "Epoch: 012/020 | Batch 220/383 | Cost: 1.4007\n",
      "Epoch: 012/020 | Batch 240/383 | Cost: 0.9581\n",
      "Epoch: 012/020 | Batch 260/383 | Cost: 1.1466\n",
      "Epoch: 012/020 | Batch 280/383 | Cost: 0.9267\n",
      "Epoch: 012/020 | Batch 300/383 | Cost: 1.1447\n",
      "Epoch: 012/020 | Batch 320/383 | Cost: 0.8921\n",
      "Epoch: 012/020 | Batch 340/383 | Cost: 0.9355\n",
      "Epoch: 012/020 | Batch 360/383 | Cost: 0.8526\n",
      "Epoch: 012/020 | Batch 380/383 | Cost: 0.8337\n",
      "base_lr:  0.09872441421741696\n",
      "Epoch: 012/020 Train Acc.: 71.76% | Validation Acc.: 64.50%\n",
      "Time elapsed: 180.32 min\n",
      "Epoch: 013/020 | Batch 000/383 | Cost: 0.8159\n",
      "Epoch: 013/020 | Batch 020/383 | Cost: 0.8496\n",
      "Epoch: 013/020 | Batch 040/383 | Cost: 0.7324\n",
      "Epoch: 013/020 | Batch 060/383 | Cost: 0.7632\n",
      "Epoch: 013/020 | Batch 080/383 | Cost: 0.7687\n",
      "Epoch: 013/020 | Batch 100/383 | Cost: 0.8665\n",
      "Epoch: 013/020 | Batch 120/383 | Cost: 0.7182\n",
      "Epoch: 013/020 | Batch 140/383 | Cost: 0.8903\n",
      "Epoch: 013/020 | Batch 160/383 | Cost: 0.9456\n",
      "Epoch: 013/020 | Batch 180/383 | Cost: 0.6751\n",
      "Epoch: 013/020 | Batch 200/383 | Cost: 0.6585\n",
      "Epoch: 013/020 | Batch 220/383 | Cost: 0.8219\n",
      "Epoch: 013/020 | Batch 240/383 | Cost: 0.7928\n",
      "Epoch: 013/020 | Batch 260/383 | Cost: 0.6477\n",
      "Epoch: 013/020 | Batch 280/383 | Cost: 0.6919\n",
      "Epoch: 013/020 | Batch 300/383 | Cost: 0.7256\n",
      "Epoch: 013/020 | Batch 320/383 | Cost: 0.7452\n",
      "Epoch: 013/020 | Batch 340/383 | Cost: 0.7505\n",
      "Epoch: 013/020 | Batch 360/383 | Cost: 0.8513\n",
      "Epoch: 013/020 | Batch 380/383 | Cost: 0.8047\n",
      "base_lr:  0.09949360580123373\n",
      "Epoch: 013/020 Train Acc.: 77.63% | Validation Acc.: 69.20%\n",
      "Time elapsed: 194.12 min\n",
      "Epoch: 014/020 | Batch 000/383 | Cost: 0.6000\n",
      "Epoch: 014/020 | Batch 020/383 | Cost: 0.6282\n",
      "Epoch: 014/020 | Batch 040/383 | Cost: 0.6469\n",
      "Epoch: 014/020 | Batch 060/383 | Cost: 0.7438\n",
      "Epoch: 014/020 | Batch 080/383 | Cost: 0.5733\n",
      "Epoch: 014/020 | Batch 100/383 | Cost: 0.7588\n",
      "Epoch: 014/020 | Batch 120/383 | Cost: 0.5982\n",
      "Epoch: 014/020 | Batch 140/383 | Cost: 0.7256\n",
      "Epoch: 014/020 | Batch 160/383 | Cost: 0.7192\n",
      "Epoch: 014/020 | Batch 180/383 | Cost: 0.7527\n",
      "Epoch: 014/020 | Batch 200/383 | Cost: 0.7716\n",
      "Epoch: 014/020 | Batch 220/383 | Cost: 0.8520\n",
      "Epoch: 014/020 | Batch 240/383 | Cost: 0.9792\n",
      "Epoch: 014/020 | Batch 260/383 | Cost: 0.7992\n",
      "Epoch: 014/020 | Batch 280/383 | Cost: 1.0161\n",
      "Epoch: 014/020 | Batch 300/383 | Cost: 0.7930\n",
      "Epoch: 014/020 | Batch 320/383 | Cost: 0.9156\n",
      "Epoch: 014/020 | Batch 340/383 | Cost: 0.6632\n",
      "Epoch: 014/020 | Batch 360/383 | Cost: 0.5783\n",
      "Epoch: 014/020 | Batch 380/383 | Cost: 0.6431\n",
      "base_lr:  0.09982435279754835\n",
      "Epoch: 014/020 Train Acc.: 79.70% | Validation Acc.: 70.90%\n",
      "Time elapsed: 207.98 min\n",
      "Epoch: 015/020 | Batch 000/383 | Cost: 0.5305\n",
      "Epoch: 015/020 | Batch 020/383 | Cost: 0.6007\n",
      "Epoch: 015/020 | Batch 040/383 | Cost: 0.5526\n",
      "Epoch: 015/020 | Batch 060/383 | Cost: 0.8430\n",
      "Epoch: 015/020 | Batch 080/383 | Cost: 0.6843\n",
      "Epoch: 015/020 | Batch 100/383 | Cost: 0.8689\n",
      "Epoch: 015/020 | Batch 120/383 | Cost: 0.6026\n",
      "Epoch: 015/020 | Batch 140/383 | Cost: 0.6896\n",
      "Epoch: 015/020 | Batch 160/383 | Cost: 0.6093\n",
      "Epoch: 015/020 | Batch 180/383 | Cost: 0.6611\n",
      "Epoch: 015/020 | Batch 200/383 | Cost: 0.8403\n",
      "Epoch: 015/020 | Batch 220/383 | Cost: 0.7128\n",
      "Epoch: 015/020 | Batch 240/383 | Cost: 0.6760\n",
      "Epoch: 015/020 | Batch 260/383 | Cost: 0.7878\n",
      "Epoch: 015/020 | Batch 280/383 | Cost: 0.8426\n",
      "Epoch: 015/020 | Batch 300/383 | Cost: 0.7325\n",
      "Epoch: 015/020 | Batch 320/383 | Cost: 0.9240\n",
      "Epoch: 015/020 | Batch 340/383 | Cost: 0.8014\n",
      "Epoch: 015/020 | Batch 360/383 | Cost: 0.7655\n",
      "Epoch: 015/020 | Batch 380/383 | Cost: 0.6004\n",
      "base_lr:  0.09994788060105264\n",
      "Epoch: 015/020 Train Acc.: 80.07% | Validation Acc.: 70.20%\n",
      "Time elapsed: 221.82 min\n",
      "Epoch: 016/020 | Batch 000/383 | Cost: 0.7193\n",
      "Epoch: 016/020 | Batch 020/383 | Cost: 0.5857\n",
      "Epoch: 016/020 | Batch 040/383 | Cost: 0.4943\n",
      "Epoch: 016/020 | Batch 060/383 | Cost: 0.7292\n",
      "Epoch: 016/020 | Batch 080/383 | Cost: 0.6303\n",
      "Epoch: 016/020 | Batch 100/383 | Cost: 0.4751\n",
      "Epoch: 016/020 | Batch 120/383 | Cost: 0.6564\n",
      "Epoch: 016/020 | Batch 140/383 | Cost: 0.7733\n",
      "Epoch: 016/020 | Batch 160/383 | Cost: 0.6063\n",
      "Epoch: 016/020 | Batch 180/383 | Cost: 0.6384\n",
      "Epoch: 016/020 | Batch 200/383 | Cost: 0.4803\n",
      "Epoch: 016/020 | Batch 220/383 | Cost: 0.8659\n",
      "Epoch: 016/020 | Batch 240/383 | Cost: 0.7310\n",
      "Epoch: 016/020 | Batch 260/383 | Cost: 0.6131\n",
      "Epoch: 016/020 | Batch 280/383 | Cost: 0.5352\n",
      "Epoch: 016/020 | Batch 300/383 | Cost: 0.5123\n",
      "Epoch: 016/020 | Batch 320/383 | Cost: 0.4907\n",
      "Epoch: 016/020 | Batch 340/383 | Cost: 0.6373\n",
      "Epoch: 016/020 | Batch 360/383 | Cost: 0.5485\n",
      "Epoch: 016/020 | Batch 380/383 | Cost: 0.7439\n",
      "base_lr:  0.09998714751994545\n",
      "Epoch: 016/020 Train Acc.: 84.77% | Validation Acc.: 72.10%\n",
      "Time elapsed: 235.66 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 017/020 | Batch 000/383 | Cost: 0.3611\n",
      "Epoch: 017/020 | Batch 020/383 | Cost: 0.5239\n",
      "Epoch: 017/020 | Batch 040/383 | Cost: 0.4786\n",
      "Epoch: 017/020 | Batch 060/383 | Cost: 0.6438\n",
      "Epoch: 017/020 | Batch 080/383 | Cost: 0.5416\n",
      "Epoch: 017/020 | Batch 100/383 | Cost: 0.5233\n",
      "Epoch: 017/020 | Batch 120/383 | Cost: 0.5968\n",
      "Epoch: 017/020 | Batch 140/383 | Cost: 0.4168\n",
      "Epoch: 017/020 | Batch 160/383 | Cost: 0.4474\n",
      "Epoch: 017/020 | Batch 180/383 | Cost: 0.4781\n",
      "Epoch: 017/020 | Batch 200/383 | Cost: 0.4947\n",
      "Epoch: 017/020 | Batch 220/383 | Cost: 0.5125\n",
      "Epoch: 017/020 | Batch 240/383 | Cost: 0.5950\n",
      "Epoch: 017/020 | Batch 260/383 | Cost: 0.4550\n",
      "Epoch: 017/020 | Batch 280/383 | Cost: 0.7764\n",
      "Epoch: 017/020 | Batch 300/383 | Cost: 0.4640\n",
      "Epoch: 017/020 | Batch 320/383 | Cost: 0.6679\n",
      "Epoch: 017/020 | Batch 340/383 | Cost: 0.5809\n",
      "Epoch: 017/020 | Batch 360/383 | Cost: 0.5011\n",
      "Epoch: 017/020 | Batch 380/383 | Cost: 0.5773\n",
      "base_lr:  0.09999747492505735\n",
      "Epoch: 017/020 Train Acc.: 86.85% | Validation Acc.: 73.30%\n",
      "Time elapsed: 249.60 min\n",
      "Epoch: 018/020 | Batch 000/383 | Cost: 0.4208\n",
      "Epoch: 018/020 | Batch 020/383 | Cost: 0.3162\n",
      "Epoch: 018/020 | Batch 040/383 | Cost: 0.3928\n",
      "Epoch: 018/020 | Batch 060/383 | Cost: 0.3715\n",
      "Epoch: 018/020 | Batch 080/383 | Cost: 0.4346\n",
      "Epoch: 018/020 | Batch 100/383 | Cost: 0.4945\n",
      "Epoch: 018/020 | Batch 120/383 | Cost: 0.3881\n",
      "Epoch: 018/020 | Batch 140/383 | Cost: 0.2886\n",
      "Epoch: 018/020 | Batch 160/383 | Cost: 0.5018\n",
      "Epoch: 018/020 | Batch 180/383 | Cost: 0.5859\n",
      "Epoch: 018/020 | Batch 200/383 | Cost: 0.3438\n",
      "Epoch: 018/020 | Batch 220/383 | Cost: 0.4645\n",
      "Epoch: 018/020 | Batch 240/383 | Cost: 0.3647\n",
      "Epoch: 018/020 | Batch 260/383 | Cost: 0.4482\n",
      "Epoch: 018/020 | Batch 280/383 | Cost: 0.4666\n",
      "Epoch: 018/020 | Batch 300/383 | Cost: 0.5431\n",
      "Epoch: 018/020 | Batch 320/383 | Cost: 0.5614\n",
      "Epoch: 018/020 | Batch 340/383 | Cost: 0.4609\n",
      "Epoch: 018/020 | Batch 360/383 | Cost: 0.5499\n",
      "Epoch: 018/020 | Batch 380/383 | Cost: 0.4899\n",
      "base_lr:  0.09999963049295997\n",
      "Epoch: 018/020 Train Acc.: 88.12% | Validation Acc.: 73.50%\n",
      "Time elapsed: 263.44 min\n",
      "Epoch: 019/020 | Batch 000/383 | Cost: 0.3208\n",
      "Epoch: 019/020 | Batch 020/383 | Cost: 0.5281\n",
      "Epoch: 019/020 | Batch 040/383 | Cost: 0.3092\n",
      "Epoch: 019/020 | Batch 060/383 | Cost: 0.4723\n",
      "Epoch: 019/020 | Batch 080/383 | Cost: 0.5025\n",
      "Epoch: 019/020 | Batch 100/383 | Cost: 0.4278\n",
      "Epoch: 019/020 | Batch 120/383 | Cost: 0.4691\n",
      "Epoch: 019/020 | Batch 140/383 | Cost: 0.4800\n",
      "Epoch: 019/020 | Batch 160/383 | Cost: 0.5908\n",
      "Epoch: 019/020 | Batch 180/383 | Cost: 0.4781\n",
      "Epoch: 019/020 | Batch 200/383 | Cost: 0.3911\n",
      "Epoch: 019/020 | Batch 220/383 | Cost: 0.2968\n",
      "Epoch: 019/020 | Batch 240/383 | Cost: 0.4110\n",
      "Epoch: 019/020 | Batch 260/383 | Cost: 0.6175\n",
      "Epoch: 019/020 | Batch 280/383 | Cost: 0.2776\n",
      "Epoch: 019/020 | Batch 300/383 | Cost: 0.3972\n",
      "Epoch: 019/020 | Batch 320/383 | Cost: 0.4292\n",
      "Epoch: 019/020 | Batch 340/383 | Cost: 0.4135\n",
      "Epoch: 019/020 | Batch 360/383 | Cost: 0.5387\n",
      "Epoch: 019/020 | Batch 380/383 | Cost: 0.4545\n",
      "base_lr:  0.09999996445187508\n",
      "Epoch: 019/020 Train Acc.: 87.25% | Validation Acc.: 70.30%\n",
      "Time elapsed: 277.33 min\n",
      "Epoch: 020/020 | Batch 000/383 | Cost: 0.4942\n",
      "Epoch: 020/020 | Batch 020/383 | Cost: 0.2149\n",
      "Epoch: 020/020 | Batch 040/383 | Cost: 0.4322\n",
      "Epoch: 020/020 | Batch 060/383 | Cost: 0.3049\n",
      "Epoch: 020/020 | Batch 080/383 | Cost: 0.3036\n",
      "Epoch: 020/020 | Batch 100/383 | Cost: 0.4020\n",
      "Epoch: 020/020 | Batch 120/383 | Cost: 0.3415\n",
      "Epoch: 020/020 | Batch 140/383 | Cost: 0.6243\n",
      "Epoch: 020/020 | Batch 160/383 | Cost: 0.4189\n",
      "Epoch: 020/020 | Batch 180/383 | Cost: 0.2790\n",
      "Epoch: 020/020 | Batch 200/383 | Cost: 0.5151\n",
      "Epoch: 020/020 | Batch 220/383 | Cost: 0.2762\n",
      "Epoch: 020/020 | Batch 240/383 | Cost: 0.3966\n",
      "Epoch: 020/020 | Batch 260/383 | Cost: 0.2862\n",
      "Epoch: 020/020 | Batch 280/383 | Cost: 0.3692\n",
      "Epoch: 020/020 | Batch 300/383 | Cost: 0.3756\n",
      "Epoch: 020/020 | Batch 320/383 | Cost: 0.4809\n",
      "Epoch: 020/020 | Batch 340/383 | Cost: 0.3816\n",
      "Epoch: 020/020 | Batch 360/383 | Cost: 0.4034\n",
      "Epoch: 020/020 | Batch 380/383 | Cost: 0.3938\n",
      "base_lr:  0.09999999836218064\n",
      "Epoch: 020/020 Train Acc.: 91.64% | Validation Acc.: 72.60%\n",
      "Time elapsed: 291.21 min\n",
      "Total Training Time: 291.21 min\n"
     ]
    }
   ],
   "source": [
    "def compute_accuracy_and_loss(model, data_loader, device):\n",
    "    correct_pred, num_examples = 0, 0\n",
    "    cross_entropy = 0.\n",
    "    for i, (features, targets) in enumerate(data_loader):\n",
    "            \n",
    "        features = features.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        outputs = model(features)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        cross_entropy += F.cross_entropy(outputs, targets)\n",
    "        num_examples += targets.size(0)\n",
    "        correct_pred += (preds == targets).sum()\n",
    "    return correct_pred.float()/num_examples * 100, cross_entropy/num_examples\n",
    "    \n",
    "collect = {'epoch': [], 'cost': [], 'train_cost':[], \n",
    "           'val_cost': [], 'train_acc': [], 'val_acc': [],\n",
    "           'learn_rate': []}\n",
    "start_time = time.time()\n",
    "train_acc_lst, test_acc_lst = [], []\n",
    "train_loss_lst, test_loss_lst = [], []\n",
    "f1_score_lst = []\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    collect['learn_rate'].append(base_lr)\n",
    "    epoch_avg_cost = 0.\n",
    "    model.train()\n",
    "    \n",
    "    for batch_idx, (features, targets) in enumerate(train_loader):\n",
    "        \n",
    "        batch_step += 1\n",
    "        ### PREPARE MINIBATCH\n",
    "        features = features.to(DEVICE)\n",
    "        targets = targets.to(DEVICE)\n",
    "            \n",
    "        ### FORWARD AND BACK PROP\n",
    "        outputs = model(features)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        cost = F.cross_entropy(outputs, targets)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        cost.backward()\n",
    "        f1_score_lst.append(f1_score(targets, preds, average = \"macro\"))\n",
    "        \n",
    "        ### UPDATE MODEL PARAMETERS\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_avg_cost += cost\n",
    "        \n",
    "        ### LOGGING\n",
    "        if not batch_idx % 20:\n",
    "            print (f'Epoch: {epoch+1:03d}/{NUM_EPOCHS:03d} | '\n",
    "                   f'Batch {batch_idx:03d}/{len(train_loader):03d} |' \n",
    "                   f' Cost: {cost:.4f}')\n",
    "\n",
    "    # no need to build the computation graph for backprop when computing accuracy\n",
    "    model.eval()\n",
    "    with torch.set_grad_enabled(False):\n",
    "        train_acc, train_loss = compute_accuracy_and_loss(model, train_loader, device=DEVICE)\n",
    "        val_acc, val_loss = compute_accuracy_and_loss(model, val_loader, device=DEVICE)\n",
    "        #test_acc, test_loss = compute_accuracy_and_loss(model, test_loader, device=DEVICE)\n",
    "        epoch_avg_cost /= batch_idx + 1\n",
    "        collect['epoch'].append(epoch+1)\n",
    "        collect['train_acc'].append(train_acc)\n",
    "        collect['train_cost'].append(train_loss)\n",
    "        collect['val_acc'].append(val_acc)\n",
    "        collect['val_cost'].append(val_loss)\n",
    "        collect['cost'].append(epoch_avg_cost / iter_per_ep)\n",
    "        print('base_lr: ', base_lr)\n",
    "        print(f'Epoch: {epoch+1:03d}/{NUM_EPOCHS:03d} Train Acc.: {train_acc:.2f}%'\n",
    "              f' | Validation Acc.: {val_acc:.2f}%')\n",
    "    \n",
    "    #############################################\n",
    "    # Update learning rate\n",
    "    base_lr = cyclical_learning_rate(batch_step=batch_step,\n",
    "                                     step_size=NUM_EPOCHS*iter_per_ep,\n",
    "                                     base_lr=base_lr,\n",
    "                                     max_lr=max_lr)\n",
    "    \n",
    "    for g in optimizer.param_groups:\n",
    "        g['lr'] = base_lr\n",
    "    #############################################\n",
    "    \n",
    "    elapsed = (time.time() - start_time)/60\n",
    "    print(f'Time elapsed: {elapsed:.2f} min')\n",
    "    \n",
    "  \n",
    "elapsed = (time.time() - start_time)/60\n",
    "print(f'Total Training Time: {elapsed:.2f} min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0a836ec4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/020 | Batch 000/383 | Cost: 2.4637\n",
      "Epoch: 001/020 | Batch 020/383 | Cost: 2.3096\n",
      "Epoch: 001/020 | Batch 040/383 | Cost: 2.2435\n",
      "Epoch: 001/020 | Batch 060/383 | Cost: 2.0148\n",
      "Epoch: 001/020 | Batch 080/383 | Cost: 2.1780\n",
      "Epoch: 001/020 | Batch 100/383 | Cost: 1.9593\n",
      "Epoch: 001/020 | Batch 120/383 | Cost: 1.7222\n",
      "Epoch: 001/020 | Batch 140/383 | Cost: 1.9668\n",
      "Epoch: 001/020 | Batch 160/383 | Cost: 1.8573\n",
      "Epoch: 001/020 | Batch 180/383 | Cost: 1.9052\n",
      "Epoch: 001/020 | Batch 200/383 | Cost: 1.7230\n",
      "Epoch: 001/020 | Batch 220/383 | Cost: 1.7497\n",
      "Epoch: 001/020 | Batch 240/383 | Cost: 1.8657\n",
      "Epoch: 001/020 | Batch 260/383 | Cost: 1.7103\n",
      "Epoch: 001/020 | Batch 280/383 | Cost: 1.9979\n",
      "Epoch: 001/020 | Batch 300/383 | Cost: 1.8365\n",
      "Epoch: 001/020 | Batch 320/383 | Cost: 1.7966\n",
      "Epoch: 001/020 | Batch 340/383 | Cost: 1.6573\n",
      "Epoch: 001/020 | Batch 360/383 | Cost: 1.7718\n",
      "Epoch: 001/020 | Batch 380/383 | Cost: 1.6084\n",
      "Epoch: 001/020 Train Acc.: 39.62% | Validation Acc.: 40.50%\n",
      "Time elapsed: 13.88 min\n",
      "Epoch: 002/020 | Batch 000/383 | Cost: 1.6618\n",
      "Epoch: 002/020 | Batch 020/383 | Cost: 1.8574\n",
      "Epoch: 002/020 | Batch 040/383 | Cost: 1.6519\n",
      "Epoch: 002/020 | Batch 060/383 | Cost: 1.6291\n",
      "Epoch: 002/020 | Batch 080/383 | Cost: 1.5178\n",
      "Epoch: 002/020 | Batch 100/383 | Cost: 1.6404\n",
      "Epoch: 002/020 | Batch 120/383 | Cost: 1.6729\n",
      "Epoch: 002/020 | Batch 140/383 | Cost: 1.5126\n",
      "Epoch: 002/020 | Batch 160/383 | Cost: 1.5248\n",
      "Epoch: 002/020 | Batch 180/383 | Cost: 1.5311\n",
      "Epoch: 002/020 | Batch 200/383 | Cost: 1.4442\n",
      "Epoch: 002/020 | Batch 220/383 | Cost: 1.4008\n",
      "Epoch: 002/020 | Batch 240/383 | Cost: 1.5004\n",
      "Epoch: 002/020 | Batch 260/383 | Cost: 1.4897\n",
      "Epoch: 002/020 | Batch 280/383 | Cost: 1.5361\n",
      "Epoch: 002/020 | Batch 300/383 | Cost: 1.3996\n",
      "Epoch: 002/020 | Batch 320/383 | Cost: 1.4372\n",
      "Epoch: 002/020 | Batch 340/383 | Cost: 1.4682\n",
      "Epoch: 002/020 | Batch 360/383 | Cost: 1.4934\n",
      "Epoch: 002/020 | Batch 380/383 | Cost: 1.4343\n",
      "Epoch: 002/020 Train Acc.: 49.57% | Validation Acc.: 48.30%\n",
      "Time elapsed: 27.75 min\n",
      "Epoch: 003/020 | Batch 000/383 | Cost: 1.5445\n",
      "Epoch: 003/020 | Batch 020/383 | Cost: 1.3804\n",
      "Epoch: 003/020 | Batch 040/383 | Cost: 1.3667\n",
      "Epoch: 003/020 | Batch 060/383 | Cost: 1.3881\n",
      "Epoch: 003/020 | Batch 080/383 | Cost: 1.3931\n",
      "Epoch: 003/020 | Batch 100/383 | Cost: 1.4674\n",
      "Epoch: 003/020 | Batch 120/383 | Cost: 1.1592\n",
      "Epoch: 003/020 | Batch 140/383 | Cost: 1.5589\n",
      "Epoch: 003/020 | Batch 160/383 | Cost: 1.3613\n",
      "Epoch: 003/020 | Batch 180/383 | Cost: 1.2996\n",
      "Epoch: 003/020 | Batch 200/383 | Cost: 1.2636\n",
      "Epoch: 003/020 | Batch 220/383 | Cost: 1.4411\n",
      "Epoch: 003/020 | Batch 240/383 | Cost: 1.1921\n",
      "Epoch: 003/020 | Batch 260/383 | Cost: 1.1573\n",
      "Epoch: 003/020 | Batch 280/383 | Cost: 1.2717\n",
      "Epoch: 003/020 | Batch 300/383 | Cost: 1.2975\n",
      "Epoch: 003/020 | Batch 320/383 | Cost: 1.4710\n",
      "Epoch: 003/020 | Batch 340/383 | Cost: 1.3040\n",
      "Epoch: 003/020 | Batch 360/383 | Cost: 1.1597\n",
      "Epoch: 003/020 | Batch 380/383 | Cost: 1.2366\n",
      "Epoch: 003/020 Train Acc.: 58.97% | Validation Acc.: 54.40%\n",
      "Time elapsed: 41.62 min\n",
      "Epoch: 004/020 | Batch 000/383 | Cost: 1.1977\n",
      "Epoch: 004/020 | Batch 020/383 | Cost: 1.2697\n",
      "Epoch: 004/020 | Batch 040/383 | Cost: 1.0504\n",
      "Epoch: 004/020 | Batch 060/383 | Cost: 1.0798\n",
      "Epoch: 004/020 | Batch 080/383 | Cost: 1.1753\n",
      "Epoch: 004/020 | Batch 100/383 | Cost: 1.1992\n",
      "Epoch: 004/020 | Batch 120/383 | Cost: 1.0121\n",
      "Epoch: 004/020 | Batch 140/383 | Cost: 1.2804\n",
      "Epoch: 004/020 | Batch 160/383 | Cost: 1.1471\n",
      "Epoch: 004/020 | Batch 180/383 | Cost: 1.1298\n",
      "Epoch: 004/020 | Batch 200/383 | Cost: 1.1715\n",
      "Epoch: 004/020 | Batch 220/383 | Cost: 1.0507\n",
      "Epoch: 004/020 | Batch 240/383 | Cost: 1.0062\n",
      "Epoch: 004/020 | Batch 260/383 | Cost: 1.1905\n",
      "Epoch: 004/020 | Batch 280/383 | Cost: 1.1012\n",
      "Epoch: 004/020 | Batch 300/383 | Cost: 1.0487\n",
      "Epoch: 004/020 | Batch 320/383 | Cost: 1.2858\n",
      "Epoch: 004/020 | Batch 340/383 | Cost: 1.0412\n",
      "Epoch: 004/020 | Batch 360/383 | Cost: 1.1133\n",
      "Epoch: 004/020 | Batch 380/383 | Cost: 1.1148\n",
      "Epoch: 004/020 Train Acc.: 66.24% | Validation Acc.: 61.80%\n",
      "Time elapsed: 55.47 min\n",
      "Epoch: 005/020 | Batch 000/383 | Cost: 1.1606\n",
      "Epoch: 005/020 | Batch 020/383 | Cost: 1.1510\n",
      "Epoch: 005/020 | Batch 040/383 | Cost: 0.9571\n",
      "Epoch: 005/020 | Batch 060/383 | Cost: 0.9165\n",
      "Epoch: 005/020 | Batch 080/383 | Cost: 0.9355\n",
      "Epoch: 005/020 | Batch 100/383 | Cost: 0.9887\n",
      "Epoch: 005/020 | Batch 120/383 | Cost: 0.8967\n",
      "Epoch: 005/020 | Batch 140/383 | Cost: 0.8355\n",
      "Epoch: 005/020 | Batch 160/383 | Cost: 1.0039\n",
      "Epoch: 005/020 | Batch 180/383 | Cost: 0.9080\n",
      "Epoch: 005/020 | Batch 200/383 | Cost: 0.8682\n",
      "Epoch: 005/020 | Batch 220/383 | Cost: 0.9468\n",
      "Epoch: 005/020 | Batch 240/383 | Cost: 0.9857\n",
      "Epoch: 005/020 | Batch 260/383 | Cost: 0.8387\n",
      "Epoch: 005/020 | Batch 280/383 | Cost: 0.7514\n",
      "Epoch: 005/020 | Batch 300/383 | Cost: 0.8765\n",
      "Epoch: 005/020 | Batch 320/383 | Cost: 0.8035\n",
      "Epoch: 005/020 | Batch 340/383 | Cost: 0.9155\n",
      "Epoch: 005/020 | Batch 360/383 | Cost: 0.8589\n",
      "Epoch: 005/020 | Batch 380/383 | Cost: 0.9682\n",
      "Epoch: 005/020 Train Acc.: 71.99% | Validation Acc.: 64.80%\n",
      "Time elapsed: 69.35 min\n",
      "Epoch: 006/020 | Batch 000/383 | Cost: 0.8232\n",
      "Epoch: 006/020 | Batch 020/383 | Cost: 0.7946\n",
      "Epoch: 006/020 | Batch 040/383 | Cost: 0.8920\n",
      "Epoch: 006/020 | Batch 060/383 | Cost: 0.9822\n",
      "Epoch: 006/020 | Batch 080/383 | Cost: 0.7854\n",
      "Epoch: 006/020 | Batch 100/383 | Cost: 0.6227\n",
      "Epoch: 006/020 | Batch 120/383 | Cost: 1.0351\n",
      "Epoch: 006/020 | Batch 140/383 | Cost: 0.7893\n",
      "Epoch: 006/020 | Batch 160/383 | Cost: 0.9270\n",
      "Epoch: 006/020 | Batch 180/383 | Cost: 0.7753\n",
      "Epoch: 006/020 | Batch 200/383 | Cost: 0.7786\n",
      "Epoch: 006/020 | Batch 220/383 | Cost: 0.7852\n",
      "Epoch: 006/020 | Batch 240/383 | Cost: 0.7485\n",
      "Epoch: 006/020 | Batch 260/383 | Cost: 1.0368\n",
      "Epoch: 006/020 | Batch 280/383 | Cost: 0.9633\n",
      "Epoch: 006/020 | Batch 300/383 | Cost: 0.8425\n",
      "Epoch: 006/020 | Batch 320/383 | Cost: 0.7717\n",
      "Epoch: 006/020 | Batch 340/383 | Cost: 0.8953\n",
      "Epoch: 006/020 | Batch 360/383 | Cost: 0.9372\n",
      "Epoch: 006/020 | Batch 380/383 | Cost: 0.8703\n",
      "Epoch: 006/020 Train Acc.: 74.10% | Validation Acc.: 67.20%\n",
      "Time elapsed: 83.24 min\n",
      "Epoch: 007/020 | Batch 000/383 | Cost: 0.9031\n",
      "Epoch: 007/020 | Batch 020/383 | Cost: 0.6748\n",
      "Epoch: 007/020 | Batch 040/383 | Cost: 0.6803\n",
      "Epoch: 007/020 | Batch 060/383 | Cost: 0.5918\n",
      "Epoch: 007/020 | Batch 080/383 | Cost: 0.7922\n",
      "Epoch: 007/020 | Batch 100/383 | Cost: 0.9725\n",
      "Epoch: 007/020 | Batch 120/383 | Cost: 0.9453\n",
      "Epoch: 007/020 | Batch 140/383 | Cost: 0.6742\n",
      "Epoch: 007/020 | Batch 160/383 | Cost: 0.7357\n",
      "Epoch: 007/020 | Batch 180/383 | Cost: 0.8357\n",
      "Epoch: 007/020 | Batch 200/383 | Cost: 0.8849\n",
      "Epoch: 007/020 | Batch 220/383 | Cost: 0.8423\n",
      "Epoch: 007/020 | Batch 240/383 | Cost: 0.6487\n",
      "Epoch: 007/020 | Batch 260/383 | Cost: 0.9295\n",
      "Epoch: 007/020 | Batch 280/383 | Cost: 0.8814\n",
      "Epoch: 007/020 | Batch 300/383 | Cost: 0.8288\n",
      "Epoch: 007/020 | Batch 320/383 | Cost: 0.7473\n",
      "Epoch: 007/020 | Batch 340/383 | Cost: 0.6530\n",
      "Epoch: 007/020 | Batch 360/383 | Cost: 0.8380\n",
      "Epoch: 007/020 | Batch 380/383 | Cost: 0.7105\n",
      "Epoch: 007/020 Train Acc.: 78.72% | Validation Acc.: 69.70%\n",
      "Time elapsed: 97.13 min\n",
      "Epoch: 008/020 | Batch 000/383 | Cost: 0.6867\n",
      "Epoch: 008/020 | Batch 020/383 | Cost: 0.6917\n",
      "Epoch: 008/020 | Batch 040/383 | Cost: 0.8263\n",
      "Epoch: 008/020 | Batch 060/383 | Cost: 0.6146\n",
      "Epoch: 008/020 | Batch 080/383 | Cost: 0.6900\n",
      "Epoch: 008/020 | Batch 100/383 | Cost: 0.6113\n",
      "Epoch: 008/020 | Batch 120/383 | Cost: 0.5489\n",
      "Epoch: 008/020 | Batch 140/383 | Cost: 0.7085\n",
      "Epoch: 008/020 | Batch 160/383 | Cost: 0.6402\n",
      "Epoch: 008/020 | Batch 180/383 | Cost: 0.5933\n",
      "Epoch: 008/020 | Batch 200/383 | Cost: 0.8115\n",
      "Epoch: 008/020 | Batch 220/383 | Cost: 0.5978\n",
      "Epoch: 008/020 | Batch 240/383 | Cost: 0.6680\n",
      "Epoch: 008/020 | Batch 260/383 | Cost: 0.7668\n",
      "Epoch: 008/020 | Batch 280/383 | Cost: 0.9329\n",
      "Epoch: 008/020 | Batch 300/383 | Cost: 0.7560\n",
      "Epoch: 008/020 | Batch 320/383 | Cost: 0.6020\n",
      "Epoch: 008/020 | Batch 340/383 | Cost: 0.6433\n",
      "Epoch: 008/020 | Batch 360/383 | Cost: 0.6811\n",
      "Epoch: 008/020 | Batch 380/383 | Cost: 0.6737\n",
      "Epoch: 008/020 Train Acc.: 80.90% | Validation Acc.: 73.00%\n",
      "Time elapsed: 111.01 min\n",
      "Epoch: 009/020 | Batch 000/383 | Cost: 0.4577\n",
      "Epoch: 009/020 | Batch 020/383 | Cost: 0.6820\n",
      "Epoch: 009/020 | Batch 040/383 | Cost: 0.6976\n",
      "Epoch: 009/020 | Batch 060/383 | Cost: 0.5802\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 009/020 | Batch 080/383 | Cost: 0.6593\n",
      "Epoch: 009/020 | Batch 100/383 | Cost: 0.3956\n",
      "Epoch: 009/020 | Batch 120/383 | Cost: 0.6518\n",
      "Epoch: 009/020 | Batch 140/383 | Cost: 0.7092\n",
      "Epoch: 009/020 | Batch 160/383 | Cost: 0.8453\n",
      "Epoch: 009/020 | Batch 180/383 | Cost: 0.6604\n",
      "Epoch: 009/020 | Batch 200/383 | Cost: 0.5712\n",
      "Epoch: 009/020 | Batch 220/383 | Cost: 0.5792\n",
      "Epoch: 009/020 | Batch 240/383 | Cost: 0.7468\n",
      "Epoch: 009/020 | Batch 260/383 | Cost: 0.6371\n",
      "Epoch: 009/020 | Batch 280/383 | Cost: 0.6878\n",
      "Epoch: 009/020 | Batch 300/383 | Cost: 0.6201\n",
      "Epoch: 009/020 | Batch 320/383 | Cost: 0.5238\n",
      "Epoch: 009/020 | Batch 340/383 | Cost: 0.6207\n",
      "Epoch: 009/020 | Batch 360/383 | Cost: 0.5814\n",
      "Epoch: 009/020 | Batch 380/383 | Cost: 0.7092\n",
      "Epoch: 009/020 Train Acc.: 82.49% | Validation Acc.: 70.50%\n",
      "Time elapsed: 124.90 min\n",
      "Epoch: 010/020 | Batch 000/383 | Cost: 0.6147\n",
      "Epoch: 010/020 | Batch 020/383 | Cost: 0.5215\n",
      "Epoch: 010/020 | Batch 040/383 | Cost: 0.6577\n",
      "Epoch: 010/020 | Batch 060/383 | Cost: 0.4352\n",
      "Epoch: 010/020 | Batch 080/383 | Cost: 0.4377\n",
      "Epoch: 010/020 | Batch 100/383 | Cost: 0.5121\n",
      "Epoch: 010/020 | Batch 120/383 | Cost: 0.5975\n",
      "Epoch: 010/020 | Batch 140/383 | Cost: 0.5875\n",
      "Epoch: 010/020 | Batch 160/383 | Cost: 0.7393\n",
      "Epoch: 010/020 | Batch 180/383 | Cost: 0.4463\n",
      "Epoch: 010/020 | Batch 200/383 | Cost: 0.5234\n",
      "Epoch: 010/020 | Batch 220/383 | Cost: 0.5095\n",
      "Epoch: 010/020 | Batch 240/383 | Cost: 0.6664\n",
      "Epoch: 010/020 | Batch 260/383 | Cost: 0.4879\n",
      "Epoch: 010/020 | Batch 280/383 | Cost: 0.6079\n",
      "Epoch: 010/020 | Batch 300/383 | Cost: 0.4700\n",
      "Epoch: 010/020 | Batch 320/383 | Cost: 0.5329\n",
      "Epoch: 010/020 | Batch 340/383 | Cost: 0.5417\n",
      "Epoch: 010/020 | Batch 360/383 | Cost: 0.5572\n",
      "Epoch: 010/020 | Batch 380/383 | Cost: 0.5488\n",
      "Epoch: 010/020 Train Acc.: 86.15% | Validation Acc.: 71.70%\n",
      "Time elapsed: 138.79 min\n",
      "Epoch: 011/020 | Batch 000/383 | Cost: 0.3578\n",
      "Epoch: 011/020 | Batch 020/383 | Cost: 0.5769\n",
      "Epoch: 011/020 | Batch 040/383 | Cost: 0.4254\n",
      "Epoch: 011/020 | Batch 060/383 | Cost: 0.4620\n",
      "Epoch: 011/020 | Batch 080/383 | Cost: 0.5604\n",
      "Epoch: 011/020 | Batch 100/383 | Cost: 0.5722\n",
      "Epoch: 011/020 | Batch 120/383 | Cost: 0.6495\n",
      "Epoch: 011/020 | Batch 140/383 | Cost: 0.4829\n",
      "Epoch: 011/020 | Batch 160/383 | Cost: 0.3612\n",
      "Epoch: 011/020 | Batch 180/383 | Cost: 0.5195\n",
      "Epoch: 011/020 | Batch 200/383 | Cost: 0.5614\n",
      "Epoch: 011/020 | Batch 220/383 | Cost: 0.6012\n",
      "Epoch: 011/020 | Batch 240/383 | Cost: 0.4622\n",
      "Epoch: 011/020 | Batch 260/383 | Cost: 0.5560\n",
      "Epoch: 011/020 | Batch 280/383 | Cost: 0.6155\n",
      "Epoch: 011/020 | Batch 300/383 | Cost: 0.4768\n",
      "Epoch: 011/020 | Batch 320/383 | Cost: 0.7196\n",
      "Epoch: 011/020 | Batch 340/383 | Cost: 0.6487\n",
      "Epoch: 011/020 | Batch 360/383 | Cost: 0.4413\n",
      "Epoch: 011/020 | Batch 380/383 | Cost: 0.5022\n",
      "Epoch: 011/020 Train Acc.: 88.26% | Validation Acc.: 74.20%\n",
      "Time elapsed: 152.66 min\n",
      "Epoch: 012/020 | Batch 000/383 | Cost: 0.5621\n",
      "Epoch: 012/020 | Batch 020/383 | Cost: 0.2906\n",
      "Epoch: 012/020 | Batch 040/383 | Cost: 0.5211\n",
      "Epoch: 012/020 | Batch 060/383 | Cost: 0.3662\n",
      "Epoch: 012/020 | Batch 080/383 | Cost: 0.5364\n",
      "Epoch: 012/020 | Batch 100/383 | Cost: 0.4874\n",
      "Epoch: 012/020 | Batch 120/383 | Cost: 0.3133\n",
      "Epoch: 012/020 | Batch 140/383 | Cost: 0.4257\n",
      "Epoch: 012/020 | Batch 160/383 | Cost: 0.4406\n",
      "Epoch: 012/020 | Batch 180/383 | Cost: 0.4834\n",
      "Epoch: 012/020 | Batch 200/383 | Cost: 0.4010\n",
      "Epoch: 012/020 | Batch 220/383 | Cost: 0.5469\n",
      "Epoch: 012/020 | Batch 240/383 | Cost: 0.5043\n",
      "Epoch: 012/020 | Batch 260/383 | Cost: 0.3961\n",
      "Epoch: 012/020 | Batch 280/383 | Cost: 0.4386\n",
      "Epoch: 012/020 | Batch 300/383 | Cost: 0.4252\n",
      "Epoch: 012/020 | Batch 320/383 | Cost: 0.4795\n",
      "Epoch: 012/020 | Batch 340/383 | Cost: 0.5795\n",
      "Epoch: 012/020 | Batch 360/383 | Cost: 0.5165\n",
      "Epoch: 012/020 | Batch 380/383 | Cost: 0.5049\n",
      "Epoch: 012/020 Train Acc.: 89.49% | Validation Acc.: 73.50%\n",
      "Time elapsed: 166.55 min\n",
      "Epoch: 013/020 | Batch 000/383 | Cost: 0.4172\n",
      "Epoch: 013/020 | Batch 020/383 | Cost: 0.2887\n",
      "Epoch: 013/020 | Batch 040/383 | Cost: 0.4440\n",
      "Epoch: 013/020 | Batch 060/383 | Cost: 0.4392\n",
      "Epoch: 013/020 | Batch 080/383 | Cost: 0.4333\n",
      "Epoch: 013/020 | Batch 100/383 | Cost: 0.3595\n",
      "Epoch: 013/020 | Batch 120/383 | Cost: 0.4490\n",
      "Epoch: 013/020 | Batch 140/383 | Cost: 0.4846\n",
      "Epoch: 013/020 | Batch 160/383 | Cost: 0.3501\n",
      "Epoch: 013/020 | Batch 180/383 | Cost: 0.4762\n",
      "Epoch: 013/020 | Batch 200/383 | Cost: 0.4473\n",
      "Epoch: 013/020 | Batch 220/383 | Cost: 0.2904\n",
      "Epoch: 013/020 | Batch 240/383 | Cost: 0.3928\n",
      "Epoch: 013/020 | Batch 260/383 | Cost: 0.4700\n",
      "Epoch: 013/020 | Batch 280/383 | Cost: 0.4923\n",
      "Epoch: 013/020 | Batch 300/383 | Cost: 0.4859\n",
      "Epoch: 013/020 | Batch 320/383 | Cost: 0.3717\n",
      "Epoch: 013/020 | Batch 340/383 | Cost: 0.3348\n",
      "Epoch: 013/020 | Batch 360/383 | Cost: 0.3488\n",
      "Epoch: 013/020 | Batch 380/383 | Cost: 0.4413\n",
      "Epoch: 013/020 Train Acc.: 90.63% | Validation Acc.: 73.20%\n",
      "Time elapsed: 180.46 min\n",
      "Epoch: 014/020 | Batch 000/383 | Cost: 0.3177\n",
      "Epoch: 014/020 | Batch 020/383 | Cost: 0.3636\n",
      "Epoch: 014/020 | Batch 040/383 | Cost: 0.2920\n",
      "Epoch: 014/020 | Batch 060/383 | Cost: 0.3507\n",
      "Epoch: 014/020 | Batch 080/383 | Cost: 0.3496\n",
      "Epoch: 014/020 | Batch 100/383 | Cost: 0.3363\n",
      "Epoch: 014/020 | Batch 120/383 | Cost: 0.4965\n",
      "Epoch: 014/020 | Batch 140/383 | Cost: 0.3958\n",
      "Epoch: 014/020 | Batch 160/383 | Cost: 0.4277\n",
      "Epoch: 014/020 | Batch 180/383 | Cost: 0.5506\n",
      "Epoch: 014/020 | Batch 200/383 | Cost: 0.4489\n",
      "Epoch: 014/020 | Batch 220/383 | Cost: 0.3427\n",
      "Epoch: 014/020 | Batch 240/383 | Cost: 0.4553\n",
      "Epoch: 014/020 | Batch 260/383 | Cost: 0.3602\n",
      "Epoch: 014/020 | Batch 280/383 | Cost: 0.3953\n",
      "Epoch: 014/020 | Batch 300/383 | Cost: 0.3407\n",
      "Epoch: 014/020 | Batch 320/383 | Cost: 0.2897\n",
      "Epoch: 014/020 | Batch 340/383 | Cost: 0.4337\n",
      "Epoch: 014/020 | Batch 360/383 | Cost: 0.4261\n",
      "Epoch: 014/020 | Batch 380/383 | Cost: 0.3934\n",
      "Epoch: 014/020 Train Acc.: 92.42% | Validation Acc.: 74.00%\n",
      "Time elapsed: 194.42 min\n",
      "Epoch: 015/020 | Batch 000/383 | Cost: 0.2196\n",
      "Epoch: 015/020 | Batch 020/383 | Cost: 0.2811\n",
      "Epoch: 015/020 | Batch 040/383 | Cost: 0.1838\n",
      "Epoch: 015/020 | Batch 060/383 | Cost: 0.3694\n",
      "Epoch: 015/020 | Batch 080/383 | Cost: 0.1997\n",
      "Epoch: 015/020 | Batch 100/383 | Cost: 0.2433\n",
      "Epoch: 015/020 | Batch 120/383 | Cost: 0.2699\n",
      "Epoch: 015/020 | Batch 140/383 | Cost: 0.4936\n",
      "Epoch: 015/020 | Batch 160/383 | Cost: 0.4988\n",
      "Epoch: 015/020 | Batch 180/383 | Cost: 0.4647\n",
      "Epoch: 015/020 | Batch 200/383 | Cost: 0.3375\n",
      "Epoch: 015/020 | Batch 220/383 | Cost: 0.3605\n",
      "Epoch: 015/020 | Batch 240/383 | Cost: 0.3677\n",
      "Epoch: 015/020 | Batch 260/383 | Cost: 0.4059\n",
      "Epoch: 015/020 | Batch 280/383 | Cost: 0.3412\n",
      "Epoch: 015/020 | Batch 300/383 | Cost: 0.2935\n",
      "Epoch: 015/020 | Batch 320/383 | Cost: 0.1890\n",
      "Epoch: 015/020 | Batch 340/383 | Cost: 0.3525\n",
      "Epoch: 015/020 | Batch 360/383 | Cost: 0.3849\n",
      "Epoch: 015/020 | Batch 380/383 | Cost: 0.3661\n",
      "Epoch: 015/020 Train Acc.: 94.02% | Validation Acc.: 74.00%\n",
      "Time elapsed: 208.32 min\n",
      "Epoch: 016/020 | Batch 000/383 | Cost: 0.3417\n",
      "Epoch: 016/020 | Batch 020/383 | Cost: 0.2719\n",
      "Epoch: 016/020 | Batch 040/383 | Cost: 0.2111\n",
      "Epoch: 016/020 | Batch 060/383 | Cost: 0.3128\n",
      "Epoch: 016/020 | Batch 080/383 | Cost: 0.2761\n",
      "Epoch: 016/020 | Batch 100/383 | Cost: 0.1984\n",
      "Epoch: 016/020 | Batch 120/383 | Cost: 0.3034\n",
      "Epoch: 016/020 | Batch 140/383 | Cost: 0.2204\n",
      "Epoch: 016/020 | Batch 160/383 | Cost: 0.4036\n",
      "Epoch: 016/020 | Batch 180/383 | Cost: 0.2298\n",
      "Epoch: 016/020 | Batch 200/383 | Cost: 0.3623\n",
      "Epoch: 016/020 | Batch 220/383 | Cost: 0.1788\n",
      "Epoch: 016/020 | Batch 240/383 | Cost: 0.4207\n",
      "Epoch: 016/020 | Batch 260/383 | Cost: 0.3315\n",
      "Epoch: 016/020 | Batch 280/383 | Cost: 0.4235\n",
      "Epoch: 016/020 | Batch 300/383 | Cost: 0.3115\n",
      "Epoch: 016/020 | Batch 320/383 | Cost: 0.2598\n",
      "Epoch: 016/020 | Batch 340/383 | Cost: 0.3429\n",
      "Epoch: 016/020 | Batch 360/383 | Cost: 0.2557\n",
      "Epoch: 016/020 | Batch 380/383 | Cost: 0.3953\n",
      "Epoch: 016/020 Train Acc.: 93.87% | Validation Acc.: 72.40%\n",
      "Time elapsed: 222.46 min\n",
      "Epoch: 017/020 | Batch 000/383 | Cost: 0.2381\n",
      "Epoch: 017/020 | Batch 020/383 | Cost: 0.2014\n",
      "Epoch: 017/020 | Batch 040/383 | Cost: 0.2109\n",
      "Epoch: 017/020 | Batch 060/383 | Cost: 0.3710\n",
      "Epoch: 017/020 | Batch 080/383 | Cost: 0.2131\n",
      "Epoch: 017/020 | Batch 100/383 | Cost: 0.2462\n",
      "Epoch: 017/020 | Batch 120/383 | Cost: 0.2152\n",
      "Epoch: 017/020 | Batch 140/383 | Cost: 0.2144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 017/020 | Batch 160/383 | Cost: 0.3594\n",
      "Epoch: 017/020 | Batch 180/383 | Cost: 0.2520\n",
      "Epoch: 017/020 | Batch 200/383 | Cost: 0.2030\n",
      "Epoch: 017/020 | Batch 220/383 | Cost: 0.4477\n",
      "Epoch: 017/020 | Batch 240/383 | Cost: 0.3188\n",
      "Epoch: 017/020 | Batch 260/383 | Cost: 0.2723\n",
      "Epoch: 017/020 | Batch 280/383 | Cost: 0.2648\n",
      "Epoch: 017/020 | Batch 300/383 | Cost: 0.3450\n",
      "Epoch: 017/020 | Batch 320/383 | Cost: 0.2261\n",
      "Epoch: 017/020 | Batch 340/383 | Cost: 0.2848\n",
      "Epoch: 017/020 | Batch 360/383 | Cost: 0.2500\n",
      "Epoch: 017/020 | Batch 380/383 | Cost: 0.2579\n",
      "Epoch: 017/020 Train Acc.: 95.16% | Validation Acc.: 73.70%\n",
      "Time elapsed: 236.41 min\n",
      "Epoch: 018/020 | Batch 000/383 | Cost: 0.2236\n",
      "Epoch: 018/020 | Batch 020/383 | Cost: 0.2018\n",
      "Epoch: 018/020 | Batch 040/383 | Cost: 0.1281\n",
      "Epoch: 018/020 | Batch 060/383 | Cost: 0.1854\n",
      "Epoch: 018/020 | Batch 080/383 | Cost: 0.2546\n",
      "Epoch: 018/020 | Batch 100/383 | Cost: 0.1990\n",
      "Epoch: 018/020 | Batch 120/383 | Cost: 0.2055\n",
      "Epoch: 018/020 | Batch 140/383 | Cost: 0.2572\n",
      "Epoch: 018/020 | Batch 160/383 | Cost: 0.2841\n",
      "Epoch: 018/020 | Batch 180/383 | Cost: 0.1916\n",
      "Epoch: 018/020 | Batch 200/383 | Cost: 0.1601\n",
      "Epoch: 018/020 | Batch 220/383 | Cost: 0.2085\n",
      "Epoch: 018/020 | Batch 240/383 | Cost: 0.3005\n",
      "Epoch: 018/020 | Batch 260/383 | Cost: 0.2335\n",
      "Epoch: 018/020 | Batch 280/383 | Cost: 0.3494\n",
      "Epoch: 018/020 | Batch 300/383 | Cost: 0.2557\n",
      "Epoch: 018/020 | Batch 320/383 | Cost: 0.2309\n",
      "Epoch: 018/020 | Batch 340/383 | Cost: 0.2865\n",
      "Epoch: 018/020 | Batch 360/383 | Cost: 0.3013\n",
      "Epoch: 018/020 | Batch 380/383 | Cost: 0.3106\n",
      "Epoch: 018/020 Train Acc.: 95.95% | Validation Acc.: 72.20%\n",
      "Time elapsed: 250.33 min\n",
      "Epoch: 019/020 | Batch 000/383 | Cost: 0.1404\n",
      "Epoch: 019/020 | Batch 020/383 | Cost: 0.1768\n",
      "Epoch: 019/020 | Batch 040/383 | Cost: 0.1823\n",
      "Epoch: 019/020 | Batch 060/383 | Cost: 0.2520\n",
      "Epoch: 019/020 | Batch 080/383 | Cost: 0.1791\n",
      "Epoch: 019/020 | Batch 100/383 | Cost: 0.1637\n",
      "Epoch: 019/020 | Batch 120/383 | Cost: 0.2474\n",
      "Epoch: 019/020 | Batch 140/383 | Cost: 0.1856\n",
      "Epoch: 019/020 | Batch 160/383 | Cost: 0.2903\n",
      "Epoch: 019/020 | Batch 180/383 | Cost: 0.2874\n",
      "Epoch: 019/020 | Batch 200/383 | Cost: 0.2947\n",
      "Epoch: 019/020 | Batch 220/383 | Cost: 0.2413\n",
      "Epoch: 019/020 | Batch 240/383 | Cost: 0.2680\n",
      "Epoch: 019/020 | Batch 260/383 | Cost: 0.1409\n",
      "Epoch: 019/020 | Batch 280/383 | Cost: 0.2784\n",
      "Epoch: 019/020 | Batch 300/383 | Cost: 0.2069\n",
      "Epoch: 019/020 | Batch 320/383 | Cost: 0.2321\n",
      "Epoch: 019/020 | Batch 340/383 | Cost: 0.2039\n",
      "Epoch: 019/020 | Batch 360/383 | Cost: 0.2324\n",
      "Epoch: 019/020 | Batch 380/383 | Cost: 0.3072\n",
      "Epoch: 019/020 Train Acc.: 96.51% | Validation Acc.: 72.40%\n",
      "Time elapsed: 264.16 min\n",
      "Epoch: 020/020 | Batch 000/383 | Cost: 0.2736\n",
      "Epoch: 020/020 | Batch 020/383 | Cost: 0.1513\n",
      "Epoch: 020/020 | Batch 040/383 | Cost: 0.0967\n",
      "Epoch: 020/020 | Batch 060/383 | Cost: 0.0748\n",
      "Epoch: 020/020 | Batch 080/383 | Cost: 0.1861\n",
      "Epoch: 020/020 | Batch 100/383 | Cost: 0.1673\n",
      "Epoch: 020/020 | Batch 120/383 | Cost: 0.2962\n",
      "Epoch: 020/020 | Batch 140/383 | Cost: 0.1213\n",
      "Epoch: 020/020 | Batch 160/383 | Cost: 0.1335\n",
      "Epoch: 020/020 | Batch 180/383 | Cost: 0.3344\n",
      "Epoch: 020/020 | Batch 200/383 | Cost: 0.3579\n",
      "Epoch: 020/020 | Batch 220/383 | Cost: 0.1311\n",
      "Epoch: 020/020 | Batch 240/383 | Cost: 0.1959\n",
      "Epoch: 020/020 | Batch 260/383 | Cost: 0.2723\n",
      "Epoch: 020/020 | Batch 280/383 | Cost: 0.3315\n",
      "Epoch: 020/020 | Batch 300/383 | Cost: 0.2225\n",
      "Epoch: 020/020 | Batch 320/383 | Cost: 0.2922\n",
      "Epoch: 020/020 | Batch 340/383 | Cost: 0.2500\n",
      "Epoch: 020/020 | Batch 360/383 | Cost: 0.2739\n",
      "Epoch: 020/020 | Batch 380/383 | Cost: 0.1599\n",
      "Epoch: 020/020 Train Acc.: 97.05% | Validation Acc.: 72.70%\n",
      "Time elapsed: 278.04 min\n",
      "Total Training Time: 278.04 min\n"
     ]
    }
   ],
   "source": [
    "## Now we test for the case when the learning rate is fixed\n",
    "\n",
    "model_fixed = EfficientNet(version = version, num_classes = NUM_CLASSES).to(DEVICE)\n",
    "model_fixed.to(DEVICE)\n",
    "\n",
    "# base_lr = 0.01\n",
    "optimizer_fixed = torch.optim.SGD(model_fixed.parameters(), lr = 0.01, momentum = 0.9)\n",
    "\n",
    "f1_score_fixed = []\n",
    "collect_fixed = {'epoch': [], 'train_cost': [], 'val_cost': [], 'train_acc': [], 'val_acc': []}\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    epoch_avg_cost_fixed = 0.\n",
    "    model_fixed.train()\n",
    "    \n",
    "    for batch_idx, (features, targets) in enumerate(train_loader):\n",
    "        \n",
    "        batch_step += 1\n",
    "        ### PREPARE MINIBATCH\n",
    "        features = features.to(DEVICE)\n",
    "        targets = targets.to(DEVICE)\n",
    "            \n",
    "        ### FORWARD AND BACK PROP\n",
    "        outputs_fixed = model_fixed(features)\n",
    "        _, preds_fixed = torch.max(outputs_fixed, 1)\n",
    "        cost_fixed = F.cross_entropy(outputs_fixed, targets)\n",
    "        optimizer_fixed.zero_grad()\n",
    "        \n",
    "        cost_fixed.backward()\n",
    "        f1_score_fixed.append(f1_score(targets, preds_fixed, average = \"macro\"))\n",
    "        \n",
    "        ### UPDATE MODEL PARAMETERS\n",
    "        optimizer_fixed.step()\n",
    "        \n",
    "        epoch_avg_cost_fixed += cost\n",
    "        \n",
    "        ### LOGGING\n",
    "        if not batch_idx % 20:\n",
    "            print (f'Epoch: {epoch+1:03d}/{NUM_EPOCHS:03d} | '\n",
    "                   f'Batch {batch_idx:03d}/{len(train_loader):03d} |' \n",
    "                   f' Cost: {cost_fixed:.4f}')\n",
    "\n",
    "    # no need to build the computation graph for backprop when computing accuracy\n",
    "    model_fixed.eval()\n",
    "    with torch.set_grad_enabled(False):\n",
    "        train_acc_fixed, train_loss_fixed = compute_accuracy_and_loss(model_fixed, train_loader, device=DEVICE)\n",
    "        val_acc_fixed, val_loss_fixed = compute_accuracy_and_loss(model_fixed, val_loader, device=DEVICE)\n",
    "        #test_acc_fixed, test_loss_fixed = compute_accuracy_and_loss(model_fixed, test_loader, device=DEVICE)\n",
    "        epoch_avg_cost_fixed /= batch_idx + 1\n",
    "        collect_fixed['epoch'].append(epoch+1)\n",
    "        collect_fixed['train_acc'].append(train_acc_fixed)\n",
    "        collect_fixed['train_cost'].append(train_loss_fixed)\n",
    "        collect_fixed['val_acc'].append(val_acc_fixed)\n",
    "        collect_fixed['val_cost'].append(val_loss_fixed)\n",
    "        #collect_fixed['cost'].append(epoch_avg_cost_fixed / iter_per_ep)\n",
    "        print(f'Epoch: {epoch+1:03d}/{NUM_EPOCHS:03d} Train Acc.: {train_acc_fixed:.2f}%'\n",
    "              f' | Validation Acc.: {val_acc_fixed:.2f}%')\n",
    "    \n",
    "    elapsed = (time.time() - start_time)/60\n",
    "    print(f'Time elapsed: {elapsed:.2f} min')\n",
    "  \n",
    "elapsed = (time.time() - start_time)/60\n",
    "print(f'Total Training Time: {elapsed:.2f} min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fa442671",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.10075082614697636,\n",
       " 0.07615214454576155,\n",
       " 0.07931357711845517,\n",
       " 0.07676007326007327,\n",
       " 0.12190870396046756,\n",
       " 0.10414271259190042,\n",
       " 0.099668607049412,\n",
       " 0.06656314699792962,\n",
       " 0.06806202305208957,\n",
       " 0.08691264804030623,\n",
       " 0.17442878291034308,\n",
       " 0.06284627403882062,\n",
       " 0.12726994651525175,\n",
       " 0.12531143457339405,\n",
       " 0.18114950743765162,\n",
       " 0.07016280451574569,\n",
       " 0.12473052016505526,\n",
       " 0.14948260766490323,\n",
       " 0.1515680248618733,\n",
       " 0.19267690152984268,\n",
       " 0.16934722524461804,\n",
       " 0.1458634007318218,\n",
       " 0.11847393208205362,\n",
       " 0.14999004376286323,\n",
       " 0.14881745418187062,\n",
       " 0.1266996279114379,\n",
       " 0.17993207941483805,\n",
       " 0.15908572981848845,\n",
       " 0.14029559790781998,\n",
       " 0.2032211955040105,\n",
       " 0.15639152514152516,\n",
       " 0.15379706640876853,\n",
       " 0.11566469027929474,\n",
       " 0.17341945363178932,\n",
       " 0.13527077937015825,\n",
       " 0.13061119989618977,\n",
       " 0.1533269476372925,\n",
       " 0.1859208693372107,\n",
       " 0.20099682015371229,\n",
       " 0.18016352946275083,\n",
       " 0.17596475586992827,\n",
       " 0.16346591096591098,\n",
       " 0.17775425723701582,\n",
       " 0.1802362147966637,\n",
       " 0.1816943531850153,\n",
       " 0.21573367808661925,\n",
       " 0.13521078714886764,\n",
       " 0.212539100333218,\n",
       " 0.18440760334057188,\n",
       " 0.19231685962720443,\n",
       " 0.1830825313718205,\n",
       " 0.19395791220665548,\n",
       " 0.23087227631471235,\n",
       " 0.18850794514410077,\n",
       " 0.20296348812477846,\n",
       " 0.2428647832892315,\n",
       " 0.2237374783777067,\n",
       " 0.20956349206349204,\n",
       " 0.23119064226481106,\n",
       " 0.2586690821256038,\n",
       " 0.26579790888339777,\n",
       " 0.3032955611598854,\n",
       " 0.22750844981219154,\n",
       " 0.16139126468974638,\n",
       " 0.20268016612380824,\n",
       " 0.1529067068197503,\n",
       " 0.16260655691690173,\n",
       " 0.25266812391922155,\n",
       " 0.25277548933495225,\n",
       " 0.26482758582673693,\n",
       " 0.17206611424410612,\n",
       " 0.13746484244029825,\n",
       " 0.2646552312681345,\n",
       " 0.23034149591870728,\n",
       " 0.22038766065883464,\n",
       " 0.26126263171143954,\n",
       " 0.17385045892804515,\n",
       " 0.21748389694041864,\n",
       " 0.24209400905053075,\n",
       " 0.18175463607406958,\n",
       " 0.22743297890351571,\n",
       " 0.3094016120282105,\n",
       " 0.24593424716213322,\n",
       " 0.19201526251526252,\n",
       " 0.25969852375399605,\n",
       " 0.21716633296616475,\n",
       " 0.23845153358497723,\n",
       " 0.25907729863105117,\n",
       " 0.2924616190823087,\n",
       " 0.20014031945066427,\n",
       " 0.27112140435526066,\n",
       " 0.2493049865789386,\n",
       " 0.2930142201628377,\n",
       " 0.25664279864279865,\n",
       " 0.27325316192705673,\n",
       " 0.2869797456977698,\n",
       " 0.3057651860078269,\n",
       " 0.29238452376939583,\n",
       " 0.2683421295694989,\n",
       " 0.2699321779879917,\n",
       " 0.30349081971507597,\n",
       " 0.3005199897553721,\n",
       " 0.3607486739839681,\n",
       " 0.36434087113397456,\n",
       " 0.3276431994935056,\n",
       " 0.34216576102674173,\n",
       " 0.3824870570010491,\n",
       " 0.3367843942147348,\n",
       " 0.21294835044835048,\n",
       " 0.27518431603570925,\n",
       " 0.24956873192167311,\n",
       " 0.36255227001194745,\n",
       " 0.299135782101246,\n",
       " 0.2854334309101751,\n",
       " 0.35823309956341115,\n",
       " 0.30832704521159526,\n",
       " 0.28897843643811383,\n",
       " 0.34260472576127227,\n",
       " 0.2680236265245753,\n",
       " 0.3360967492260062,\n",
       " 0.3203520177433221,\n",
       " 0.3153693860804845,\n",
       " 0.24436601307189543,\n",
       " 0.3043679273942432,\n",
       " 0.3158203105803725,\n",
       " 0.35978466580976365,\n",
       " 0.2803075889488933,\n",
       " 0.2862050028019644,\n",
       " 0.2872344301705356,\n",
       " 0.2979885932609732,\n",
       " 0.31719842099152445,\n",
       " 0.29018476525720155,\n",
       " 0.3256504057786529,\n",
       " 0.2697469088165183,\n",
       " 0.2892255892255892,\n",
       " 0.3560898049582863,\n",
       " 0.2972307181962355,\n",
       " 0.3501119685467512,\n",
       " 0.25947669288235853,\n",
       " 0.26308742299136856,\n",
       " 0.23776528707563188,\n",
       " 0.29707625904210344,\n",
       " 0.3094065964218765,\n",
       " 0.3229479962465443,\n",
       " 0.336022004311478,\n",
       " 0.2559159101767797,\n",
       " 0.3010279426831151,\n",
       " 0.3081640773020083,\n",
       " 0.24436420957879582,\n",
       " 0.3496808666373884,\n",
       " 0.2943594947513712,\n",
       " 0.3134697894697895,\n",
       " 0.332990462266778,\n",
       " 0.26566948530012824,\n",
       " 0.27211102211102206,\n",
       " 0.2664723529904354,\n",
       " 0.252861169837914,\n",
       " 0.35786802628266046,\n",
       " 0.21901686295667142,\n",
       " 0.2957053660911001,\n",
       " 0.2595588655290758,\n",
       " 0.2822524947552757,\n",
       " 0.28051989386004733,\n",
       " 0.23566301441882836,\n",
       " 0.2818319759192277,\n",
       " 0.24519449630257464,\n",
       " 0.30629513637078587,\n",
       " 0.25209769543757965,\n",
       " 0.22726111287086898,\n",
       " 0.2694767638376661,\n",
       " 0.2731154627208276,\n",
       " 0.2896902352033931,\n",
       " 0.2961192009376326,\n",
       " 0.2984690041755259,\n",
       " 0.2923899868247694,\n",
       " 0.3320595394943221,\n",
       " 0.32717949157677506,\n",
       " 0.34007584120027895,\n",
       " 0.2546779747777933,\n",
       " 0.3424977035348552,\n",
       " 0.3236574536574537,\n",
       " 0.3442927612625386,\n",
       " 0.29662470580948846,\n",
       " 0.3517624328379477,\n",
       " 0.2841019492841125,\n",
       " 0.3147144062458464,\n",
       " 0.33389363242304415,\n",
       " 0.3443535274062418,\n",
       " 0.33095503398134973,\n",
       " 0.29949363884658,\n",
       " 0.2643942686792585,\n",
       " 0.3126404833991041,\n",
       " 0.2735742020822666,\n",
       " 0.33799819135723763,\n",
       " 0.3913994592623625,\n",
       " 0.3431969944469944,\n",
       " 0.2845355847985581,\n",
       " 0.34136908253037285,\n",
       " 0.26063638736693767,\n",
       " 0.3112463535132201,\n",
       " 0.3832438547528062,\n",
       " 0.35941789918576217,\n",
       " 0.34124404269565556,\n",
       " 0.36164893617161176,\n",
       " 0.3715691735914902,\n",
       " 0.27563118022916655,\n",
       " 0.3380753003474441,\n",
       " 0.349586186801704,\n",
       " 0.2611472130020517,\n",
       " 0.3522979746481868,\n",
       " 0.2696011973902441,\n",
       " 0.3055982512111544,\n",
       " 0.26805709780301434,\n",
       " 0.3559795595931668,\n",
       " 0.30990488006617045,\n",
       " 0.32892314450156496,\n",
       " 0.38173879142300193,\n",
       " 0.3320992360466045,\n",
       " 0.34811687748770315,\n",
       " 0.42029345654345657,\n",
       " 0.2893502224333867,\n",
       " 0.4074264815644126,\n",
       " 0.30287584528761,\n",
       " 0.29562161599732756,\n",
       " 0.3000418494808396,\n",
       " 0.316257176947879,\n",
       " 0.3129392732024311,\n",
       " 0.3895955843897105,\n",
       " 0.34219618616677444,\n",
       " 0.3321077847056666,\n",
       " 0.36987186895418267,\n",
       " 0.34884946180471965,\n",
       " 0.3064206482522155,\n",
       " 0.2898211875843455,\n",
       " 0.31812890393535553,\n",
       " 0.3437953687258694,\n",
       " 0.39584708778257166,\n",
       " 0.3139345228274448,\n",
       " 0.34093554474925936,\n",
       " 0.32518777204685534,\n",
       " 0.2981475710744003,\n",
       " 0.30500424412189114,\n",
       " 0.297858229498696,\n",
       " 0.3631163379371183,\n",
       " 0.3747139478791021,\n",
       " 0.3358779225546577,\n",
       " 0.3406830621704764,\n",
       " 0.3867123830692047,\n",
       " 0.39782748969042914,\n",
       " 0.4094177088484868,\n",
       " 0.32269110066866025,\n",
       " 0.3464107352342647,\n",
       " 0.3671597015410403,\n",
       " 0.33338525650594614,\n",
       " 0.348008658008658,\n",
       " 0.3641104287380585,\n",
       " 0.3267917533424593,\n",
       " 0.36633433770530544,\n",
       " 0.33268369175627244,\n",
       " 0.3868861578619239,\n",
       " 0.3574583895636527,\n",
       " 0.346861158165506,\n",
       " 0.27561782889369096,\n",
       " 0.4320101535891009,\n",
       " 0.4383901172994317,\n",
       " 0.39477069020679567,\n",
       " 0.3105789132850602,\n",
       " 0.3723423808817753,\n",
       " 0.2935940454893943,\n",
       " 0.32265311500094107,\n",
       " 0.35044965512070775,\n",
       " 0.30049860250775584,\n",
       " 0.3331600212652844,\n",
       " 0.3340037851428905,\n",
       " 0.2901136426297717,\n",
       " 0.38303191414697807,\n",
       " 0.30303147037592315,\n",
       " 0.37249664061428767,\n",
       " 0.27061754767053336,\n",
       " 0.3697499560587131,\n",
       " 0.2857844164058586,\n",
       " 0.2395061605061605,\n",
       " 0.3119110945813732,\n",
       " 0.26021612465090727,\n",
       " 0.25908968499680574,\n",
       " 0.33777746111364443,\n",
       " 0.26340273350719595,\n",
       " 0.27365938305968285,\n",
       " 0.270876135358894,\n",
       " 0.23853046402924064,\n",
       " 0.3196620297265459,\n",
       " 0.3222998642923075,\n",
       " 0.2367973237965248,\n",
       " 0.27321374666202247,\n",
       " 0.29524614843116076,\n",
       " 0.28687555097958817,\n",
       " 0.24693307188001587,\n",
       " 0.29550893608701545,\n",
       " 0.2703486317603964,\n",
       " 0.27785804789752155,\n",
       " 0.28337898463653244,\n",
       " 0.2715319865319865,\n",
       " 0.2778504088504089,\n",
       " 0.3231031746031746,\n",
       " 0.2746721332158196,\n",
       " 0.2783090144095836,\n",
       " 0.2982089175192624,\n",
       " 0.27886243386243387,\n",
       " 0.25037205081669694,\n",
       " 0.23057485886433254,\n",
       " 0.3586546379335753,\n",
       " 0.3286387686387687,\n",
       " 0.2929045172538046,\n",
       " 0.279838612067445,\n",
       " 0.28256631098187074,\n",
       " 0.2671556982823641,\n",
       " 0.29821298508255023,\n",
       " 0.3059364905521185,\n",
       " 0.3116493920222551,\n",
       " 0.3447307295400531,\n",
       " 0.34605738654937734,\n",
       " 0.3494907407407407,\n",
       " 0.35393284715607,\n",
       " 0.3170740563784042,\n",
       " 0.26872868778954534,\n",
       " 0.24512099833455334,\n",
       " 0.2295954389015134,\n",
       " 0.2634894717600562,\n",
       " 0.27750942684766217,\n",
       " 0.3314112393242828,\n",
       " 0.37496805754077117,\n",
       " 0.2944998135560749,\n",
       " 0.3683114021993332,\n",
       " 0.32048709444677187,\n",
       " 0.3236725434275344,\n",
       " 0.30991156240649137,\n",
       " 0.3079558123036384,\n",
       " 0.33275345362301884,\n",
       " 0.3539263319800404,\n",
       " 0.2978969617755225,\n",
       " 0.38177152362276356,\n",
       " 0.3247565773792983,\n",
       " 0.35274684437775644,\n",
       " 0.35516702359021973,\n",
       " 0.3123011652626257,\n",
       " 0.39982700504951146,\n",
       " 0.34819721718088326,\n",
       " 0.36894604212253873,\n",
       " 0.3137344159803484,\n",
       " 0.35125091991741436,\n",
       " 0.29125138660737215,\n",
       " 0.3077928879344451,\n",
       " 0.3274578677136817,\n",
       " 0.2708809731390377,\n",
       " 0.3860674718858862,\n",
       " 0.4010367140674046,\n",
       " 0.34369922565722805,\n",
       " 0.3377713920817369,\n",
       " 0.40327935962274364,\n",
       " 0.4005038981758548,\n",
       " 0.3244744938268908,\n",
       " 0.3781875113190903,\n",
       " 0.28178247758433517,\n",
       " 0.40511444111444106,\n",
       " 0.31562286810570567,\n",
       " 0.32363749432657685,\n",
       " 0.33014688144649484,\n",
       " 0.3623477848987594,\n",
       " 0.3135782591554705,\n",
       " 0.3831213090868263,\n",
       " 0.3531116908404538,\n",
       " 0.42828065866096426,\n",
       " 0.3364504168838534,\n",
       " 0.3856358962866193,\n",
       " 0.35564471490277944,\n",
       " 0.3579620730147046,\n",
       " 0.36000085630333556,\n",
       " 0.3793864843229634,\n",
       " 0.3230776272985742,\n",
       " 0.3310188094462288,\n",
       " 0.40375151668255116,\n",
       " 0.3726796288003185,\n",
       " 0.30963500749710204,\n",
       " 0.3775421366874711,\n",
       " 0.29537650835476925,\n",
       " 0.4512474498681396,\n",
       " 0.28343792172739535,\n",
       " 0.3860500475755145,\n",
       " 0.40588962511751125,\n",
       " 0.31905196206932757,\n",
       " 0.3147267183585969,\n",
       " 0.3833217221375116,\n",
       " 0.32095127095127096,\n",
       " 0.2799584064101496,\n",
       " 0.3278382520610874,\n",
       " 0.27900642535874914,\n",
       " 0.38362293643094414,\n",
       " 0.3458753043458926,\n",
       " 0.350740928810348,\n",
       " 0.32549558735157114,\n",
       " 0.3947342572667954,\n",
       " 0.27146799060211624,\n",
       " 0.2811814503960367,\n",
       " 0.2837401357041537,\n",
       " 0.31947962915397815,\n",
       " 0.42730750770182435,\n",
       " 0.29416982221036947,\n",
       " 0.2805009086973012,\n",
       " 0.2948787722609233,\n",
       " 0.34636336586707744,\n",
       " 0.26799050192948637,\n",
       " 0.3223275360117466,\n",
       " 0.3498235512224912,\n",
       " 0.3646448748081609,\n",
       " 0.3901028433151845,\n",
       " 0.29169672758803195,\n",
       " 0.3201704286038456,\n",
       " 0.32401234924764344,\n",
       " 0.36396508950819295,\n",
       " 0.3289701385527157,\n",
       " 0.2686360984108395,\n",
       " 0.32125451793620674,\n",
       " 0.31277685593293586,\n",
       " 0.40889301039754755,\n",
       " 0.29147423016381924,\n",
       " 0.3298261257370413,\n",
       " 0.30658471774183843,\n",
       " 0.3191326496161138,\n",
       " 0.21744689039186196,\n",
       " 0.40766681734201204,\n",
       " 0.30268905174452393,\n",
       " 0.3913598044255939,\n",
       " 0.27570099193382946,\n",
       " 0.2754954514084949,\n",
       " 0.3529810467310467,\n",
       " 0.34111294917746526,\n",
       " 0.3021648960179695,\n",
       " 0.47276222049879346,\n",
       " 0.4357962907962908,\n",
       " 0.3773296878560037,\n",
       " 0.26876345876345875,\n",
       " 0.4572689708656159,\n",
       " 0.36249439950892903,\n",
       " 0.35464345378791384,\n",
       " 0.348514528620857,\n",
       " 0.3401482767019851,\n",
       " 0.4175747376201329,\n",
       " 0.33218719211822656,\n",
       " 0.32775273269663174,\n",
       " 0.3800479046043562,\n",
       " 0.38225867039139355,\n",
       " 0.3380711761356922,\n",
       " 0.4124684765911974,\n",
       " 0.3687123351256211,\n",
       " 0.3682704366279683,\n",
       " 0.33734821955538064,\n",
       " 0.3367861305361305,\n",
       " 0.42730192110626897,\n",
       " 0.4023040124041629,\n",
       " 0.3578888316396659,\n",
       " 0.38569567674427013,\n",
       " 0.36589953999970987,\n",
       " 0.33746283268622096,\n",
       " 0.4474200903533495,\n",
       " 0.46260993392572336,\n",
       " 0.39103943873737923,\n",
       " 0.38267702268123027,\n",
       " 0.39003442366260954,\n",
       " 0.40528131118668204,\n",
       " 0.2935829548026344,\n",
       " 0.42720332736252986,\n",
       " 0.366350454803558,\n",
       " 0.4354767890062007,\n",
       " 0.3245884251571858,\n",
       " 0.4211102099715858,\n",
       " 0.4038056798533657,\n",
       " 0.4294581517331942,\n",
       " 0.4818161300912426,\n",
       " 0.46013430774721104,\n",
       " 0.42037147029887495,\n",
       " 0.42383239906860065,\n",
       " 0.41534790571747093,\n",
       " 0.382052943717843,\n",
       " 0.4179063282284698,\n",
       " 0.3777664492370375,\n",
       " 0.4169335047289359,\n",
       " 0.35515298063165573,\n",
       " 0.4048544061302682,\n",
       " 0.35388499470770834,\n",
       " 0.32297376122503996,\n",
       " 0.38431010383951564,\n",
       " 0.39238819875776404,\n",
       " 0.47063173838774464,\n",
       " 0.36405993706414463,\n",
       " 0.36820019969270773,\n",
       " 0.37783028809144426,\n",
       " 0.37766616098291816,\n",
       " 0.37707462211937914,\n",
       " 0.30162561511179004,\n",
       " 0.2792387307093189,\n",
       " 0.37693911829768834,\n",
       " 0.4464478504313675,\n",
       " 0.3670583727743971,\n",
       " 0.3482371489381181,\n",
       " 0.3874638595005357,\n",
       " 0.38662823529356816,\n",
       " 0.3747890116656102,\n",
       " 0.3591952377947929,\n",
       " 0.35048811920595846,\n",
       " 0.3211016921528601,\n",
       " 0.3669091009410809,\n",
       " 0.3004091841911642,\n",
       " 0.3573086698247988,\n",
       " 0.3248769675333143,\n",
       " 0.3938066371519185,\n",
       " 0.33131658690245025,\n",
       " 0.3656147667513237,\n",
       " 0.403008948267569,\n",
       " 0.36245831040138443,\n",
       " 0.32723450751449135,\n",
       " 0.44630930897059934,\n",
       " 0.4067035432026943,\n",
       " 0.38003830592065885,\n",
       " 0.3521334841628959,\n",
       " 0.3406611029191674,\n",
       " 0.35956260508063165,\n",
       " 0.3685577260759958,\n",
       " 0.34526364777886076,\n",
       " 0.3315743248089159,\n",
       " 0.35619939038417303,\n",
       " 0.3953746207530904,\n",
       " 0.3883840436472015,\n",
       " 0.4173924671565324,\n",
       " 0.3649557131672881,\n",
       " 0.4106372735417224,\n",
       " 0.4435174502916438,\n",
       " 0.38615564012865355,\n",
       " 0.31272572906314855,\n",
       " 0.47373305398305393,\n",
       " 0.46147688002404025,\n",
       " 0.33136042159655,\n",
       " 0.4089222522117259,\n",
       " 0.49881028070266825,\n",
       " 0.4267326065118935,\n",
       " 0.4149438673632223,\n",
       " 0.3775353535353535,\n",
       " 0.4129944664599396,\n",
       " 0.37912964396922416,\n",
       " 0.41265965897544843,\n",
       " 0.3511723646723647,\n",
       " 0.4087274969923712,\n",
       " 0.4681231917026384,\n",
       " 0.42292797561927264,\n",
       " 0.3676710762355924,\n",
       " 0.47535782549512523,\n",
       " 0.34466156264877496,\n",
       " 0.4605948022092523,\n",
       " 0.37518511903186824,\n",
       " 0.4160117986588575,\n",
       " 0.3636595479104519,\n",
       " 0.3740126247810741,\n",
       " 0.44735266533220497,\n",
       " 0.411410003319716,\n",
       " 0.38932387912325267,\n",
       " 0.36847049689441,\n",
       " 0.37344408869311285,\n",
       " 0.431192290377645,\n",
       " 0.46899726915855944,\n",
       " 0.42019081380038,\n",
       " 0.4313810436983897,\n",
       " 0.4226109941081478,\n",
       " 0.4489429804328605,\n",
       " 0.459768518198658,\n",
       " 0.42291571604334577,\n",
       " 0.3814157857476272,\n",
       " 0.40317008883725525,\n",
       " 0.41842668726785137,\n",
       " 0.46071434604043304,\n",
       " 0.44668158917161077,\n",
       " 0.4146410965547121,\n",
       " 0.4073116993086033,\n",
       " 0.3472651570274661,\n",
       " 0.47373132241553295,\n",
       " 0.4361140456647517,\n",
       " 0.48585706101835135,\n",
       " 0.4737319421425945,\n",
       " 0.3632746641442294,\n",
       " 0.4005356430062313,\n",
       " 0.3760182408795602,\n",
       " 0.46674324264160927,\n",
       " 0.4064159514159513,\n",
       " 0.4242598039215687,\n",
       " 0.44237276482814314,\n",
       " 0.4009009985779496,\n",
       " 0.41483627483627483,\n",
       " 0.4037293233082707,\n",
       " 0.3856510934960255,\n",
       " 0.4922357573625649,\n",
       " 0.43028835582208896,\n",
       " 0.3905741050093808,\n",
       " 0.4539355175784542,\n",
       " 0.44631333676003704,\n",
       " 0.43475843040189394,\n",
       " 0.43570657773878346,\n",
       " 0.4859350183285274,\n",
       " 0.39329337017157195,\n",
       " 0.4154061887900363,\n",
       " 0.4536815224369404,\n",
       " 0.4570362140975246,\n",
       " 0.36150495481529965,\n",
       " 0.41029960572386937,\n",
       " 0.4872541811800867,\n",
       " 0.4074755177568477,\n",
       " 0.5084069749845732,\n",
       " 0.4409726312193959,\n",
       " 0.42275284900284904,\n",
       " 0.4140893368250939,\n",
       " 0.44469005922099225,\n",
       " 0.4180082018707245,\n",
       " 0.33239064139521807,\n",
       " 0.41369429931498897,\n",
       " 0.4668192852898736,\n",
       " 0.37627705627705627,\n",
       " 0.41594757749691114,\n",
       " 0.41245818726674643,\n",
       " 0.4695002823556047,\n",
       " 0.39060892268622227,\n",
       " 0.47512138866276316,\n",
       " 0.43506731381605246,\n",
       " 0.40545963359756465,\n",
       " 0.3783432303456115,\n",
       " 0.42132501757501756,\n",
       " 0.4779329979438433,\n",
       " 0.49297213263127143,\n",
       " 0.4153627830258265,\n",
       " 0.48467490688117215,\n",
       " 0.41670256358112423,\n",
       " 0.45437557063926237,\n",
       " 0.45419985811290164,\n",
       " 0.4079408602906045,\n",
       " 0.5164522144522146,\n",
       " 0.4408700811146956,\n",
       " 0.4573815200694583,\n",
       " 0.4412409520304257,\n",
       " 0.40783092724269193,\n",
       " 0.43150775902206107,\n",
       " 0.3661971745219827,\n",
       " 0.40249807849807856,\n",
       " 0.4748212724432236,\n",
       " 0.4771980993247021,\n",
       " 0.43670914230696845,\n",
       " 0.4369552467949937,\n",
       " 0.41685567497636467,\n",
       " 0.3990538513815717,\n",
       " 0.4290096863333039,\n",
       " 0.4726727721941771,\n",
       " 0.4805264339747099,\n",
       " 0.40748526155967174,\n",
       " 0.4588729551412136,\n",
       " 0.4575887095736138,\n",
       " 0.38000530429323,\n",
       " 0.4200651833963486,\n",
       " 0.41192760496745606,\n",
       " 0.419219767511345,\n",
       " 0.4937644408082697,\n",
       " 0.5259561355076751,\n",
       " 0.5066897013504355,\n",
       " 0.4100133900396525,\n",
       " 0.46444623655913986,\n",
       " 0.42591393338418515,\n",
       " 0.4259718365925262,\n",
       " 0.4800201589242098,\n",
       " 0.4761445554234928,\n",
       " 0.5067911072474967,\n",
       " 0.4484867871391641,\n",
       " 0.43980192284943936,\n",
       " 0.46909549171376036,\n",
       " 0.5205390708564168,\n",
       " 0.42192932557638435,\n",
       " 0.4099053614408305,\n",
       " 0.41948055565384,\n",
       " 0.3986229000884174,\n",
       " 0.40860025062656635,\n",
       " 0.5151915287441603,\n",
       " 0.48691850131754116,\n",
       " 0.4360587197349406,\n",
       " 0.43424198470750197,\n",
       " 0.4842134882661198,\n",
       " 0.4862128307030855,\n",
       " 0.4204425360284527,\n",
       " 0.4216178191620378,\n",
       " 0.5151053056065844,\n",
       " 0.40064855883746436,\n",
       " 0.4870532493934028,\n",
       " 0.44017713655335083,\n",
       " 0.47500631171049285,\n",
       " 0.41477364366157465,\n",
       " 0.49232077550968106,\n",
       " 0.4493927607431501,\n",
       " 0.4358939525947199,\n",
       " 0.5231196349507544,\n",
       " 0.44936770211770216,\n",
       " 0.5078051296529558,\n",
       " 0.5208770181486899,\n",
       " 0.47909646587674937,\n",
       " 0.4154550008595185,\n",
       " 0.4523224732941447,\n",
       " 0.47295680368166704,\n",
       " 0.44038641054507927,\n",
       " 0.45534466296480025,\n",
       " 0.4565568083032586,\n",
       " 0.4388381296276034,\n",
       " 0.3538140238940922,\n",
       " 0.3590623982890243,\n",
       " 0.4845718459977119,\n",
       " 0.41522023065501334,\n",
       " 0.5287687170239765,\n",
       " 0.40705356472090753,\n",
       " 0.5263759910937331,\n",
       " 0.4151777771342989,\n",
       " 0.4937375669870926,\n",
       " 0.4572098250296592,\n",
       " 0.46093833427799275,\n",
       " 0.41750465221053457,\n",
       " 0.450680777939255,\n",
       " 0.4995783037968196,\n",
       " 0.5176558065956631,\n",
       " 0.4149161175300726,\n",
       " 0.45762362637362636,\n",
       " 0.48382193332614093,\n",
       " 0.4495718136721528,\n",
       " 0.48159120730378635,\n",
       " 0.44006499726327314,\n",
       " 0.4568688477294658,\n",
       " 0.5112998085792204,\n",
       " 0.4864209217834977,\n",
       " 0.4247183580449301,\n",
       " 0.4335998034089398,\n",
       " 0.48292579841301064,\n",
       " 0.5694619315928688,\n",
       " 0.47521755833313295,\n",
       " 0.5137151260016372,\n",
       " 0.4912919450310754,\n",
       " 0.4491132306469498,\n",
       " 0.4132098477207172,\n",
       " 0.41758766212522025,\n",
       " 0.4553275335775336,\n",
       " 0.42955218955218955,\n",
       " 0.5398010121923166,\n",
       " 0.44285064360926424,\n",
       " 0.49895783360937873,\n",
       " 0.4668250710686751,\n",
       " 0.45820579558399743,\n",
       " 0.49722018695156234,\n",
       " 0.4994436857192016,\n",
       " 0.47089256924954864,\n",
       " 0.45807776547311435,\n",
       " 0.43418653315205036,\n",
       " 0.5417765131524722,\n",
       " 0.5038821371416606,\n",
       " 0.4951978021978022,\n",
       " 0.4645635840539268,\n",
       " 0.41061011016274174,\n",
       " 0.44739671955940735,\n",
       " 0.42925231638874817,\n",
       " 0.413873082808441,\n",
       " 0.38607347231722355,\n",
       " 0.4599105780718684,\n",
       " 0.4363762420957543,\n",
       " 0.49031434576890814,\n",
       " 0.5653578783077935,\n",
       " 0.4263555590229018,\n",
       " 0.4808338654099524,\n",
       " 0.5037062277023914,\n",
       " 0.4871241792639965,\n",
       " 0.5011713109083307,\n",
       " 0.5109854184627718,\n",
       " 0.4824775498253759,\n",
       " 0.44599843674843676,\n",
       " 0.507484706158595,\n",
       " 0.49747720234676757,\n",
       " 0.48579938088476987,\n",
       " 0.47818908140100475,\n",
       " 0.5095918793849827,\n",
       " 0.4782818560749595,\n",
       " 0.4932628062936592,\n",
       " 0.47355513304067254,\n",
       " 0.5972110169299075,\n",
       " 0.5009218559218559,\n",
       " 0.5461564297771194,\n",
       " 0.5816718459640248,\n",
       " 0.5742205776810844,\n",
       " 0.4642256604616601,\n",
       " 0.4659480626819457,\n",
       " 0.45642974798035973,\n",
       " 0.4918409391008328,\n",
       " 0.5554499064154237,\n",
       " 0.3930397325294304,\n",
       " 0.4475235291024765,\n",
       " 0.4452533827034282,\n",
       " 0.4845009109664282,\n",
       " 0.4800538965942046,\n",
       " 0.4253452029641288,\n",
       " 0.528087144469986,\n",
       " 0.436352309970731,\n",
       " 0.483164743752979,\n",
       " 0.4756582789191485,\n",
       " 0.4163494551425585,\n",
       " 0.5244888159849254,\n",
       " 0.45900965395232507,\n",
       " 0.546742927639512,\n",
       " 0.48417863352917784,\n",
       " 0.48447792152012525,\n",
       " 0.48315937973780027,\n",
       " 0.5912485992399984,\n",
       " 0.501806178445853,\n",
       " 0.42271211547073617,\n",
       " 0.4419927373742724,\n",
       " 0.4110238633301699,\n",
       " 0.45429897716662426,\n",
       " 0.469856208887649,\n",
       " 0.5196873186550606,\n",
       " 0.41459787339097687,\n",
       " 0.4976066162722835,\n",
       " 0.5187652687396932,\n",
       " 0.49386413151119035,\n",
       " 0.5115132041300023,\n",
       " 0.48674930321953863,\n",
       " 0.5251616127843308,\n",
       " 0.5073619131115383,\n",
       " 0.5328000190126481,\n",
       " 0.5004993870658594,\n",
       " 0.5006759450234824,\n",
       " 0.579123219579609,\n",
       " 0.4512318688928179,\n",
       " 0.5137885814983175,\n",
       " 0.48468072864657313,\n",
       " 0.49365352476980384,\n",
       " 0.5126863354037268,\n",
       " 0.5106378284033763,\n",
       " 0.49017974308188333,\n",
       " 0.47567495992850956,\n",
       " 0.4768927414120994,\n",
       " 0.471081485830872,\n",
       " 0.5288476770018352,\n",
       " 0.5659658459300816,\n",
       " 0.49871705790184057,\n",
       " 0.5019081337342206,\n",
       " 0.4666427097806408,\n",
       " 0.5223914757930104,\n",
       " 0.5797939847280661,\n",
       " 0.5681376695761832,\n",
       " 0.5243106078898505,\n",
       " 0.44764880979423705,\n",
       " 0.5332448292448293,\n",
       " 0.5546563601517162,\n",
       " 0.468569579845442,\n",
       " 0.4557520936384233,\n",
       " 0.49081124062528236,\n",
       " 0.5321477508813438,\n",
       " 0.4117776341305753,\n",
       " 0.47421435536226825,\n",
       " 0.48462519408633203,\n",
       " 0.5286038788463887,\n",
       " 0.5348368780977477,\n",
       " 0.5454031294792164,\n",
       " 0.515262383737225,\n",
       " 0.49377037455298317,\n",
       " 0.49229641575793737,\n",
       " 0.5793954518104667,\n",
       " 0.4654069940138156,\n",
       " 0.5352172546948143,\n",
       " 0.5356560680698611,\n",
       " 0.49837206867466327,\n",
       " 0.46460172349815815,\n",
       " 0.49672126520423737,\n",
       " 0.5485645297205399,\n",
       " 0.4906945646945647,\n",
       " 0.43836663336663334,\n",
       " 0.5137848749325554,\n",
       " 0.5240761298945442,\n",
       " 0.5903989053336118,\n",
       " 0.44639034878165323,\n",
       " 0.5707659954483911,\n",
       " 0.583894586894587,\n",
       " 0.6017679332162091,\n",
       " 0.5126400482154957,\n",
       " 0.5670346689400398,\n",
       " 0.5677913735439972,\n",
       " 0.48022089504698207,\n",
       " 0.4572749116551921,\n",
       " 0.5050334512712797,\n",
       " 0.510837041428693,\n",
       " 0.45137842714073617,\n",
       " 0.49558617164787766,\n",
       " 0.501120140644855,\n",
       " 0.5581623144509392,\n",
       " 0.5196163444398738,\n",
       " 0.4962229898811608,\n",
       " 0.5316637409251629,\n",
       " 0.49775724275724276,\n",
       " 0.5150313019575586,\n",
       " 0.4838640478514621,\n",
       " 0.4717338217338217,\n",
       " 0.5082734691212952,\n",
       " 0.5362193362193362,\n",
       " 0.43973836199642646,\n",
       " 0.5026178063274838,\n",
       " 0.49045960340943895,\n",
       " 0.5423268801829522,\n",
       " 0.522140332812967,\n",
       " 0.46244501580364955,\n",
       " 0.4769362931215829,\n",
       " 0.5361941219699842,\n",
       " 0.5418142867098481,\n",
       " 0.538380098797719,\n",
       " 0.4349244468503345,\n",
       " 0.48851611840191084,\n",
       " 0.53313077407905,\n",
       " 0.4857523137781758,\n",
       " 0.495668137183067,\n",
       " 0.4510062010062009,\n",
       " 0.4568225472517198,\n",
       " 0.4860761790803868,\n",
       " 0.5603823953823953,\n",
       " 0.45654892174655715,\n",
       " 0.46729965456711425,\n",
       " 0.5237956061651714,\n",
       " 0.49615084915084917,\n",
       " 0.5732297704632908,\n",
       " 0.48317710515291157,\n",
       " 0.58531798314407,\n",
       " 0.5788577002707437,\n",
       " 0.48610734307508496,\n",
       " 0.5336345304221892,\n",
       " 0.5117119029891263,\n",
       " 0.5819593774541302,\n",
       " 0.594495656848598,\n",
       " 0.517012247012247,\n",
       " 0.578981351981352,\n",
       " 0.5478192136953747,\n",
       " 0.4861293480572419,\n",
       " 0.491471788949103,\n",
       " 0.6057650015544752,\n",
       " 0.5305528883909694,\n",
       " 0.47126461769115446,\n",
       " 0.5145390444579087,\n",
       " 0.4900158102766798,\n",
       " 0.515395123978965,\n",
       " 0.5548719522463438,\n",
       " 0.5147008169796775,\n",
       " 0.4535150970808866,\n",
       " 0.49253884711779455,\n",
       " 0.5415868945868946,\n",
       " 0.5173129114345967,\n",
       " 0.4994183606820125,\n",
       " 0.5208225108225109,\n",
       " 0.5153797033107377,\n",
       " 0.5793327362292879,\n",
       " 0.533161502425772,\n",
       " 0.5241015747537486,\n",
       " 0.5724682944820245,\n",
       " 0.5215465509149719,\n",
       " 0.611265708187905,\n",
       " 0.5731241289178557,\n",
       " 0.5289252424822293,\n",
       " 0.5200155141895965,\n",
       " 0.5057738323058016,\n",
       " 0.6130073162193036,\n",
       " 0.44420441627338186,\n",
       " 0.47924675324675314,\n",
       " 0.47788717892938265,\n",
       " 0.53042176525235,\n",
       " 0.4959439560238108,\n",
       " 0.4737751793396955,\n",
       " 0.5002490812669841,\n",
       " 0.5478050798344336,\n",
       " 0.5441320437741531,\n",
       " 0.46799929401542306,\n",
       " 0.5458222605281429,\n",
       " 0.5218257082891808,\n",
       " 0.5234581280788178,\n",
       " 0.5362645477009733,\n",
       " 0.5666554172963469,\n",
       " 0.53642618301239,\n",
       " 0.42669928084021036,\n",
       " 0.4345447766092928,\n",
       " 0.5589130227754074,\n",
       " 0.5090742519318199,\n",
       " 0.5070725885097338,\n",
       " 0.5483193511594633,\n",
       " 0.5361150384171133,\n",
       " 0.532968507257407,\n",
       " 0.5138018898664061,\n",
       " 0.5099803478672157,\n",
       " 0.5730746191165749,\n",
       " 0.44976182504155354,\n",
       " 0.5527326921444569,\n",
       " 0.5004793252724287,\n",
       " 0.5268038040864128,\n",
       " ...]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "06b08343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 73.08%\n"
     ]
    }
   ],
   "source": [
    "# Test accuracy\n",
    "\n",
    "model.eval()\n",
    "with torch.set_grad_enabled(False): # save memory during inference\n",
    "    test_acc, test_loss = compute_accuracy_and_loss(model, test_loader, DEVICE)\n",
    "    print(f'Test accuracy: {test_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aebc39a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 73.59%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.set_grad_enabled(False): # save memory during inference\n",
    "    test_acc, test_loss = compute_accuracy_and_loss(model_fixed, test_loader, DEVICE)\n",
    "    print(f'Test accuracy: {test_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b04cba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
